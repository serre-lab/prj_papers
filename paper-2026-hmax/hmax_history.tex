\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{natbib}
\usepackage{geometry}
\geometry{margin=1in}

% Title information
\title{Notes on HMAX}
\author{Thomas Serre}
\date{\today}

\begin{document}

\maketitle

\subsection{Hierarchical models of the visual cortex}

A foundational insight for hierarchical models of vision originates in the classic work of Hubel and Wiesel, who demonstrated that receptive fields in the early visual system are organized along two linked progressions. First, there is a progression in \emph{stimulus complexity}, from center--surround responses in the retina and lateral geniculate nucleus (LGN) to oriented, phase-sensitive simple cells in primary visual cortex (V1), and onward to neurons selective for conjunctions of oriented elements. Second, there is a progression in \emph{invariance}, whereby simple cells respond to specific orientations at specific positions, while complex cells pool over similarly tuned simple cells to achieve tolerance to stimulus position and phase \citep{HubelWiesel1962}. This dual hierarchy of increasing selectivity and increasing tolerance provided the conceptual blueprint for extending the simple--complex architecture beyond V1 to the broader ventral visual stream. Anatomical studies established that areas along the ventral pathway (V1, V2, V4, and inferotemporal cortex, IT) form a serially organized hierarchy \citep{FellemanVanEssen1991}, while physiological work demonstrated a corresponding increase in both the complexity of preferred stimuli and tolerance to identity-preserving transformations such as changes in position and scale \citep{Logothetis1995,DiCarlo2012}. Fukushima’s Neocognitron \citep{Fukushima1980} was the first computational model to operationalize this principle, implementing an alternating cascade of S- and C-like layers that progressively increase the complexity and invariance of visual representations.

Several biologically motivated hierarchical models expanded on these ideas by explicitly linking neurophysiological findings along the ventral stream with computational principles of object recognition. Influential theoretical work proposed that invariant object representations could be learned from transformation sequences through temporal association mechanisms \citep{Foldiak1991,PoggioEdelman1990}. Perrett and colleagues, drawing on extensive single-unit recordings in inferior temporal cortex, proposed conceptual hierarchies in which increasingly ``elaborate'' feature-selective cells emerge through successive stages of convergence from earlier shape-selective units \citep{PerrettOram1993,Tanaka1996}. In parallel, Rolls and collaborators developed multilayer networks---later referred to as VisNet---that implemented a ventral-stream--like hierarchy and introduced temporally contiguous ``trace'' learning rules to acquire invariant object and face representations from natural visual experience \citep{WallisRolls1997,Rolls2012}. These models emphasized three organizational principles that continue to shape contemporary theories of the ventral stream: (i) a hierarchical progression of selectivity and invariance, (ii) pooling over progressively larger receptive fields, and (iii) learning mechanisms that exploit the temporal continuity of natural viewing.

A key empirical motivation for quantitative hierarchical models of invariant object recognition came from the paperclip experiments of Logothetis et al.\ (1995) \citep{Logothetis1995}. In these studies, monkeys were trained to recognize novel three-dimensional ``paperclip'' objects across limited rotations in depth, with each object presented during training at a single retinal position and a fixed size. Single-unit recordings in anterior IT revealed neurons that were strongly tuned to particular trained objects and specific views in depth, with tuning curves typically spanning $\sim20^\circ$ of rotation. Critically, when the same objects were tested at new retinal positions and different sizes, many of these neurons nonetheless exhibited substantial tolerance to position (several degrees of visual angle) and scale (approximately two octaves), despite the fact that these transformations had not been experienced during training. While some position tolerance could in principle be acquired through eye movements during learning, the scale tolerance is more difficult to explain in this way: because viewing distance was fixed, the monkeys could not have experienced the objects at different retinal sizes. This pattern provides strong evidence that tolerance to two-dimensional transformations such as position and scale does not arise de novo for each new object, but instead builds on a pre-existing, generic invariant representation available in IT prior to object-specific learning \citep{LogothetisSheinberg1996}.

Riesenhuber and Poggio’s HMAX model \citep{RiesenhuberPoggio1999} was explicitly designed to provide a quantitative account of these IT response properties. HMAX instantiated a ventral-stream--like hierarchy of alternating simple (S) and complex (C) layers, in which S units increase feature selectivity through template matching, while C units pool over afferent units with similar selectivity to increase tolerance to two-dimensional transformations. In this framework, generic invariance to position and scale is built gradually through cascaded pooling operations across layers (from S1/C1 to S2/C2), independently of any particular object. Object- and view-specific selectivity in IT is then modeled by higher-level S units that combine inputs from these already invariant representations. A central prediction of the model was that complex cells implement a max-like pooling operation. This prediction was later tested directly by Lampl, Riesenhuber, Poggio, and colleagues \citep{Lampl2004}, who showed through intracellular recordings in cat V1 that complex-cell membrane potentials are well described by a max-like nonlinearity over their simple-cell inputs. Together, these results provided strong empirical support for the core computational mechanisms underlying the HMAX framework.

Building on HMAX, Serre and colleagues extended the framework into a quantitatively specified, feedforward model of the ventral stream that was more tightly constrained by available anatomy and physiology \citep{Serre2007}. Relative to the original formulation, these implementations emphasized matching early-stage parameters to neurophysiological measurements, including receptive-field sizes, spatial-frequency tuning, orientation bandwidths, pooling ranges, and retinotopic organization in V1 and V2, with intermediate stages consistent with V4 and posterior inferotemporal cortex. In addition, the model incorporated simple task-independent learning to acquire a reusable dictionary of intermediate features from natural images, rather than relying on hand-designed mid-level templates. Complementary theoretical work demonstrated that both selectivity and invariance could in principle be learned directly from natural video streams via temporal association rules, providing a plausibility proof for experience-driven development of complex-cell-like responses \citep{Masquelier2007tr}.

Historically, these design choices reflected both biological constraints and the state of machine learning at the time. Large-scale end-to-end supervised training via backpropagation was not only computationally difficult but also widely viewed as biologically implausible; influential critiques emphasized that backpropagation required unrealistic mechanisms for credit assignment and weight transport \citep{Crick1989}. This motivated strong interest in local and layerwise learning strategies, including deep belief networks and greedy layerwise pretraining \citep{Hinton2006,Bengio2007,Larochelle2009}. In parallel, work from LeCun’s group showed that convolutional architectures trained end-to-end by backpropagation could dramatically overfit in the small-sample regime: on Caltech-101 with 30 training examples per class, a supervised convnet achieved near-perfect training accuracy but only $\sim$20\% test performance, whereas layerwise unsupervised feature learning yielded substantially higher accuracy \citep{Ranzato2007}. This context helps explain why, in the mid-2000s, biologically inspired hierarchical models with simple local learning rules could be competitive---and in some cases superior---to end-to-end trained convnets on realistic recognition benchmarks with limited labeled data.

Empirically, the resulting Serre et al.\ model family was validated at multiple levels. On the engineering side, the model achieved strong performance on object recognition benchmarks and was competitive with state-of-the-art computer vision systems of the era, including on Caltech-101 \citep{Serre2005cvpr,Serre2007pami}. On the neuroscience side, the same framework supported quantitative comparisons to neural data and provided a coherent account of ``immediate recognition'' in the feedforward sweep \citep{Serre2007}. Critically, Serre, Oliva, and Poggio (PNAS 2007) demonstrated that a feedforward implementation could predict human performance in rapid animal-versus-non-animal categorization---including systematic effects of clutter and masking---supporting the hypothesis that the first $\sim$100--150 ms of visual recognition can be explained largely without invoking top-down feedback \citep{Serre2007pnas,Thorpe1996}.

\subsection{From hierarchical models to deep learning}

The modern deep learning revolution in vision is often traced to the ImageNet Large Scale Visual Recognition Challenge, where AlexNet produced a dramatic jump in performance and established large-scale supervised training of convolutional networks as a dominant paradigm \citep{Krizhevsky2012,Russakovsky2015}. Subsequent architectural advances made it possible to train much deeper networks: VGG and GoogLeNet (Inception) were among the first widely used very deep models \citep{Simonyan2015,Szegedy2015}, and residual connections then enabled extremely deep architectures such as ResNet-152 \citep{He2016}. As reviewed in \citet{Serre2019ARVS}, these advances led to rapid improvements in ImageNet accuracy, with some models achieving nominally human-level or superhuman performance on the benchmark. Importantly, these gains were obtained on large-scale internet image datasets and do not correspond to rapid visual categorization paradigms designed to isolate the feedforward sweep.

A major reason these advances mattered for neuroscience is that early improvements in task performance were often accompanied by improved alignment with neural and behavioral data. Seminal work by Yamins and colleagues showed that convolutional networks optimized for object recognition could quantitatively predict neural responses in macaque IT and V4, outperforming earlier biologically inspired models and supporting the task-optimization hypothesis \citep{Yamins2014,Cadieu2014}. This perspective was later formalized in the Brain-Score framework, which established standardized benchmarks for comparing models against neural and behavioral data and showed that early gains in ImageNet performance were frequently mirrored by gains in neural predictivity \citep{Schrimpf2020}. Subsequent studies refined this picture by examining how architectural choices, training objectives, and depth influence alignment with cortical representations \citep{KhalighRazavi2014}.

Recent work, however, has refined both the empirical benchmarks and the interpretation of ``human-level'' performance. As discussed in \citet{LinsleyFengSerreTICS}, evaluation on a cleaned, multi-label version of ImageNet---paired with accuracy estimates from trained human annotators---provides a principled reference point for human performance. Using this benchmark, that work conducts statistical tests to quantify the fraction of modern vision models (drawn from the TIMM model zoo) whose performance falls within, or exceeds, the human range. These results show that while many contemporary architectures achieve nominally human-level or superhuman accuracy on ImageNet, such gains do not necessarily translate into corresponding improvements in neural or perceptual alignment. We return to these issues in the Discussion and refer the reader to \citet{LinsleyFengSerreTICS} for a detailed analysis.

% Bibliography
\bibliographystyle{plain}
\bibliography{neurips_2025}

\end{document}