@article{colorconchrom,
	author = {Chakrabarti, Ayan},
	month = {06},
	title = {Color Constancy by Learning to Predict Chromaticity from Luminance},
	year = {2015}}

@misc{ravi2024sam2segmentimages,
      title={SAM 2: Segment Anything in Images and Videos}, 
      author={Nikhila Ravi and Valentin Gabeur and Yuan-Ting Hu and Ronghang Hu and Chaitanya Ryali and Tengyu Ma and Haitham Khedr and Roman Rädle and Chloe Rolland and Laura Gustafson and Eric Mintun and Junting Pan and Kalyan Vasudev Alwala and Nicolas Carion and Chao-Yuan Wu and Ross Girshick and Piotr Dollár and Christoph Feichtenhofer},
      year={2024},
      eprint={2408.00714},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.00714}, 
}
@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}
@misc{zhang2025evfsamearlyvisionlanguagefusion,
      title={EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything Model}, 
      author={Yuxuan Zhang and Tianheng Cheng and Lianghui Zhu and Rui Hu and Lei Liu and Heng Liu and Longjin Ran and Xiaoxin Chen and Wenyu Liu and Xinggang Wang},
      year={2025},
      eprint={2406.20076},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.20076}, 
}
@misc{qian2020benchmark,
      title={A Benchmark for Temporal Color Constancy}, 
      author={Yanlin Qian and Jani Käpylä and Joni-Kristian Kämäräinen and Samu Koskinen and Jiri Matas},
      year={2020},
      eprint={2003.03763},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{Ciurea2003ALI,
  title={A Large Image Database for Color Constancy Research},
  author={Florian Ciurea and Brian V. Funt},
  booktitle={Color Imaging Conference},
  year={2003}
}
@article{ElShamayleh2016Curvature,
  author  = {El-Shamayleh, Yasmine and Pasupathy, Anitha},
  title   = {Contour Curvature as an Invariant Code for Objects in Visual Area V4},
  journal = {Journal of Neuroscience},
  year    = {2016},
  volume  = {36},
  number  = {20},
  pages   = {5532--5543},
  doi     = {10.1523/JNEUROSCI.4139-15.2016},
  url     = {https://www.jneurosci.org/content/36/20/5532}
}

@article{Han2020,
  title   = {Scale and translation-invariance for novel objects in human vision},
  author  = {Han, Yena and Roig, Gemma and Geiger, Gad and Poggio, Tomaso},
  journal = {Scientific Reports},
  volume  = {10},
  number  = {1},
  pages   = {1411},
  year    = {2020},
  doi     = {10.1038/s41598-019-57261-6}
}

@article{Schrimpf2018BrainScore,
  author  = {Schrimpf, Martin and Kubilius, Jonas and Lee, Michael J. and Ratan Murty, N. Anish and Ajemian, Robert and DiCarlo, James J.},
  title   = {Brain-Score: Which Artificial Neural Network for Object Recognition Is Most Brain-Like?},
  journal = {bioRxiv},
  pages   = {407007},
  year    = {2018},
  doi     = {10.1101/407007},
  url     = {https://www.biorxiv.org/content/10.1101/407007v1}
}

@article{Singh2017AnAO,
  title={An Analysis of Scale Invariance in Object Detection - SNIP},
  author={Bharat Singh and Larry S. Davis},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={3578-3587}
}
@article{Zhang2019,
author = {Zhang, Jiaxuan and Han, Yena and Poggio, Tomaso and Roig, Gemma},
year = {2019},
month = {09},
pages = {209},
title = {Eccentricity Dependent Neural Network with Recurrent Attention for Scale, Translation and Clutter Invariance},
volume = {19},
journal = {Journal of Vision},
doi = {10.1167/19.10.209}
}
@INPROCEEDINGS{deng,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@inproceedings{Simonyan2015,
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author    = {Simonyan, Karen and Zisserman, Andrew},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2015},
  url       = {https://arxiv.org/abs/1409.1556}
}

@ARTICLE{Bowers2022-cy,
  title    = "Deep Problems with Neural Network Models of Human Vision",
  author   = "Bowers, Jeffrey S and Malhotra, Gaurav and Dujmovi{\'c}, Marin
              and Montero, Milton Llera and Tsvetkov, Christian and Biscione,
              Valerio and Puebla, Guillermo and Adolfi, Federico and Hummel,
              John E and Heaton, Rachel F and Evans, Benjamin D and Mitchell,
              Jeffrey and Blything, Ryan",
  abstract = "Deep neural networks (DNNs) have had extraordinary successes in
              classifying photographic images of objects and are often
              described as the best models of biological vision. This
              conclusion is largely based on three sets of findings: (1) DNNs
              are more accurate than any other model in classifying images
              taken from various datasets, (2) DNNs do the best job in
              predicting the pattern of human errors in classifying objects
              taken from various behavioral datasets, and (3) DNNs do the best
              job in predicting brain signals in response to images taken from
              various brain datasets (e.g., single cell responses or fMRI
              data). However, these behavioral and brain datasets do not test
              hypotheses regarding what features are contributing to good
              predictions and we show that the predictions may be mediated by
              DNNs that share little overlap with biological vision. More
              problematically, we show that DNNs account for almost no results
              from psychological research. This contradicts the common claim
              that DNNs are good, let alone the best, models of human object
              recognition. We argue that theorists interested in developing
              biologically plausible models of human vision need to direct
              their attention to explaining psychological findings. More
              generally, theorists need to build models that explain the
              results of experiments that manipulate independent variables
              designed to test hypotheses rather than compete on making the
              best predictions. We conclude by briefly summarizing various
              promising modelling approaches that focus on psychological data.",
  journal  = "Behav. Brain Sci.",
  pages    = "1--74",
  month    =  dec,
  year     =  2022,
  keywords = "Brain-Score; Computational Neuroscience; Deep Neural Networks;
              Human Vision; Object recognition",
  language = "en"
}

@ARTICLE{Majaj2015-bb,
  title    = "Simple Learned Weighted Sums of Inferior Temporal Neuronal Firing
              Rates Accurately Predict Human Core Object Recognition
              Performance",
  author   = "Majaj, Najib J and Hong, Ha and Solomon, Ethan A and DiCarlo,
              James J",
  abstract = "To go beyond qualitative models of the biological substrate of
              object recognition, we ask: can a single ventral stream neuronal
              linking hypothesis quantitatively account for core object
              recognition performance over a broad range of tasks? We measured
              human performance in 64 object recognition tests using thousands
              of challenging images that explore shape similarity and identity
              preserving object variation. We then used multielectrode arrays
              to measure neuronal population responses to those same images in
              visual areas V4 and inferior temporal (IT) cortex of monkeys and
              simulated V1 population responses. We tested leading candidate
              linking hypotheses and control hypotheses, each postulating how
              ventral stream neuronal responses underlie object recognition
              behavior. Specifically, for each hypothesis, we computed the
              predicted performance on the 64 tests and compared it with the
              measured pattern of human performance. All tested hypotheses
              based on low- and mid-level visually evoked activity (pixels, V1,
              and V4) were very poor predictors of the human behavioral
              pattern. However, simple learned weighted sums of distributed
              average IT firing rates exactly predicted the behavioral pattern.
              More elaborate linking hypotheses relying on IT trial-by-trial
              correlational structure, finer IT temporal codes, or ones that
              strictly respect the known spatial substructures of IT (``face
              patches'') did not improve predictive power. Although these
              results do not reject those more elaborate hypotheses, they
              suggest a simple, sufficient quantitative model: each object
              recognition task is learned from the spatially distributed mean
              firing rates (100 ms) of ∼60,000 IT neurons and is executed as a
              simple weighted sum of those firing rates. Significance
              statement: We sought to go beyond qualitative models of visual
              object recognition and determine whether a single neuronal
              linking hypothesis can quantitatively account for core object
              recognition behavior. To achieve this, we designed a database of
              images for evaluating object recognition performance. We used
              multielectrode arrays to characterize hundreds of neurons in the
              visual ventral stream of nonhuman primates and measured the
              object recognition performance of >100 human observers.
              Remarkably, we found that simple learned weighted sums of firing
              rates of neurons in monkey inferior temporal (IT) cortex
              accurately predicted human performance. Although previous work
              led us to expect that IT would outperform V4, we were surprised
              by the quantitative precision with which simple IT-based linking
              hypotheses accounted for human behavior.",
  journal  = "J. Neurosci.",
  volume   =  35,
  number   =  39,
  pages    = "13402--13418",
  month    =  sep,
  year     =  2015,
  keywords = "IT cortex; V4; categorization; identification; invariance; object
              recognition",
  language = "en"
}
@misc{linsley2023performanceoptimized,
      title={Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex}, 
      author={Drew Linsley and Ivan F. Rodriguez and Thomas Fel and Michael Arcaro and Saloni Sharma and Margaret Livingstone and Thomas Serre},
      year={2023},
      eprint={2306.03779},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@ARTICLE{Fel2022-og,
  title   = "Harmonizing the object recognition strategies of deep neural
             networks with humans",
  author  = "Fel*, Thomas and Felipe*, Ivan and Linsley*, Drew and Serre,
             Thomas",
  journal = "Adv. Neural Inf. Process. Syst.",
  year    =  2022
}

@ARTICLE{Schrimpf2020-az,
  title    = "Integrative Benchmarking to Advance Neurally Mechanistic Models
              of Human Intelligence",
  author   = "Schrimpf, Martin and Kubilius, Jonas and Lee, Michael J and Ratan
              Murty, N Apurva and Ajemian, Robert and DiCarlo, James J",
  abstract = "A potentially organizing goal of the brain and cognitive sciences
              is to accurately explain domains of human intelligence as
              executable, neurally mechanistic models. Years of research have
              led to models that capture experimental results in individual
              behavioral tasks and individual brain regions. We here advocate
              for taking the next step: integrating experimental results from
              many laboratories into suites of benchmarks that, when considered
              together, push mechanistic models toward explaining entire
              domains of intelligence, such as vision, language, and motor
              control. Given recent successes of neurally mechanistic models
              and the surging availability of neural, anatomical, and
              behavioral data, we believe that now is the time to create
              integrative benchmarking platforms that incentivize ambitious,
              unified models. This perspective discusses the advantages and the
              challenges of this approach and proposes specific steps to
              achieve this goal in the domain of visual intelligence with the
              case study of an integrative benchmarking platform called
              Brain-Score.",
  journal  = "Neuron",
  volume   =  108,
  number   =  3,
  pages    = "413--423",
  month    =  nov,
  year     =  2020,
  keywords = "computational neuroscience; integrative benchmarking; neurally
              mechanistic modeling; ventral stream",
  language = "en"
}



@article{columns_barlow,
	abstract = {Image processing requires free access to information about all parts of an image, but a nerve cell in V1 can only interact directly with a tiny fraction of the other cells in V1. The problem this poses might be alleviated by forming secondary "neural images" in which information is re-arranged, and some possible rules of projection for forming such images are explored. It is also suggested that all parts of the cerebral cortex detect, and subsequently signal, suspicious coincidences in their inputs. Acquiring knowledge of the associative structure of sensory messages, in the form of the unexpected coincidences that occur, may represent the beginning of the formation of a working model, or cognitive map, of the environment.},
	author = {Barlow, H B},
	crdt = {1986/01/01 00:00},
	date = {1986},
	date-added = {2022-10-28 00:51:04 -0400},
	date-modified = {2022-10-28 00:51:04 -0400},
	dcom = {19860630},
	doi = {10.1016/0042-6989(86)90072-6},
	edat = {1986/01/01 00:00},
	issn = {0042-6989 (Print); 0042-6989 (Linking)},
	jid = {0417402},
	journal = {Vision Res},
	jt = {Vision research},
	language = {eng},
	lr = {20190728},
	mh = {Brain Mapping; Form Perception/physiology; Humans; Motion Perception/physiology; Neural Pathways; Neurons/physiology; Pattern Recognition, Visual/physiology; Visual Cortex/*physiology},
	mhda = {1986/01/01 00:01},
	number = {1},
	own = {NLM},
	pages = {81--90},
	phst = {1986/01/01 00:00 {$[$}pubmed{$]$}; 1986/01/01 00:01 {$[$}medline{$]$}; 1986/01/01 00:00 {$[$}entrez{$]$}},
	pii = {0042-6989(86)90072-6},
	pl = {England},
	pmid = {3716216},
	pst = {ppublish},
	pt = {Journal Article},
	sb = {IM},
	status = {MEDLINE},
	title = {Why have multiple cortical areas?},
	volume = {26},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1016/0042-6989(86)90072-6}}


@article{mehrani_multiplicative_2020,
	abstract = {There is still much to understand about the brain's colour processing mechanisms and the transformation from cone-opponent representations to perceptual hues. Moreover, it is unclear which area(s) in the brain represent unique hues. We propose a hierarchical model inspired by the neuronal mechanisms in the brain for local hue representation, which reveals the contributions of each visual cortical area in hue representation. Hue encoding is achieved through incrementally increasing processing nonlinearities beginning with cone input. Besides employing nonlinear rectifications, we propose multiplicative modulations as a form of nonlinearity. Our simulation results indicate that multiplicative modulations have significant contributions in encoding of hues along intermediate directions in the MacLeod-Boynton diagram and that our model V2 neurons have the capacity to encode unique hues. Additionally, responses of our model neurons resemble those of biological colour cells, suggesting that our model provides a novel formulation of the brain's colour processing pathway.},
	author = {Mehrani, Paria and Mouraviev, Andrei and Tsotsos, John K.},
	doi = {10.1038/s41598-020-64969-3},
	issn = {2045-2322},
	journal = {Scientific Reports},
	month = may,
	number = {1},
	pages = {8491},
	title = {Multiplicative modulations enhance diversity of hue-selective cells},
	url = {https://doi.org/10.1038/s41598-020-64969-3},
	volume = {10},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-020-64969-3}}

@article{Kraft307,
	abstract = {Color constancy is our ability to perceive constant surface colors despite changes in illumination. Although color constancy has been studied extensively, its mechanisms are still largely unknown. Three classic hypotheses are that constancy is mediated by local adaptation, by adaptation to the spatial mean of the image, or by adaptation to the most intense image region. We measure color constancy under nearly natural viewing conditions, by using a design that allows us to test these three hypotheses directly. By suitable stimulus manipulation, we are able to titrate the degree of constancy between 11\% and 83\%, indicating that we have achieved good laboratory control. Our results rule out all three classic hypotheses and thus suggest that there is more to constancy than can be easily explained by the action of simple visual mechanisms. CIE,Commission Internationale de l{\textquoteright}Eclairage},
	author = {Kraft, J. M. and Brainard, D. H.},
	doi = {10.1073/pnas.96.1.307},
	eprint = {https://www.pnas.org/content/96/1/307.full.pdf},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
	number = {1},
	pages = {307--312},
	publisher = {National Academy of Sciences},
	title = {Mechanisms of color constancy under nearly natural viewing},
	url = {https://www.pnas.org/content/96/1/307},
	volume = {96},
	year = {1999},
	bdsk-url-1 = {https://www.pnas.org/content/96/1/307},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.96.1.307}}

@article{Cheng:14,
	abstract = {Color constancy is a well-studied topic in color vision. Methods are generally categorized as (1)\&\#xA0;low-level statistical methods, (2)\&\#xA0;gamut-based methods, and (3)\&\#xA0;learning-based methods. In this work, we distinguish methods depending on whether they work directly from color values (i.e., color domain) or from values obtained from the image\&\#x2019;s spatial information (e.g., image gradients/frequencies). We show that spatial information does not provide any additional information that cannot be obtained directly from the color distribution and that the indirect aim of spatial-domain methods is to obtain large color differences for estimating the illumination direction. This finding allows us to develop a simple and efficient illumination estimation method that chooses bright and dark pixels using a projection distance in the color distribution and then applies principal component analysis to estimate the illumination direction. Our method gives state-of-the-art results on existing public color constancy datasets as well as on our newly collected dataset (NUS dataset) containing 1736 images from eight different high-end consumer cameras.},
	author = {Dongliang Cheng and Dilip K. Prasad and Michael S. Brown},
	doi = {10.1364/JOSAA.31.001049},
	journal = {J. Opt. Soc. Am. A},
	keywords = {Cameras; Illumination; Color vision; Color, rendering and metamerism ; Color constancy; Color difference; Color vision; Image processing; Visual system; White light},
	month = {May},
	number = {5},
	pages = {1049--1058},
	publisher = {OSA},
	title = {Illuminant estimation for color constancy: why spatial-domain methods work and the role of the color distribution},
	url = {http://opg.optica.org/josaa/abstract.cfm?URI=josaa-31-5-1049},
	volume = {31},
	year = {2014},
	bdsk-url-1 = {http://opg.optica.org/josaa/abstract.cfm?URI=josaa-31-5-1049},
	bdsk-url-2 = {https://doi.org/10.1364/JOSAA.31.001049}}

@INPROCEEDINGS{gehlerdataset,
  author={Gehler, Peter Vincent and Rother, Carsten and Blake, Andrew and Minka, Tom and Sharp, Toby},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Bayesian color constancy revisited}, 
  year={2008},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/CVPR.2008.4587765}}
@article{Wong2006,
abstract = {Recent physiological studies using behaving monkeys revealed that, in a two-alternative forced-choice visual motion discrimination task, reaction time was correlated with ramping of spike activity of lateral intraparietal cortical neurons. The ramping activity appears to reflect temporal accumulation, on a timescale of hundreds of milliseconds, of sensory evidence before a decision is reached. To elucidate the cellular and circuit basis of such integration times, we developed and investigated a simplified two-variable version of a biophysically realistic cortical network model of decision making. In this model, slow time integration can be achieved robustly if excitatory reverberation is primarily mediated by NMDA receptors; our model with only fast AMPA receptors at recurrent synapses produces decision times that are not comparable with experimental observations. Moreover, we found two distinct modes of network behavior, in which decision computation by winner-take-all competition is instantiated with or without attractor states for working memory. Decision process is closely linked to the local dynamics, in the "decision space" of the system, in the vicinity of an unstable saddle steady state that separates the basins of attraction for the two alternative choices. This picture provides a rigorous and quantitative explanation for the dependence of performance and response time on the degree of task difficulty, and the reason for which reaction times are longer in error trials than in correct trials as observed in the monkey experiment. Our reduced two-variable neural model offers a simple yet biophysically plausible framework for studying perceptual decision making in general. Copyright {\textcopyright} 2006 Society for Neuroscience.},
author = {Wong, Kong Fatt and Wang, Xiao Jing},
doi = {10.1523/JNEUROSCI.3733-05.2006},
file = {:Users/pipemac/Downloads/wang{\_}rnn{\_}Circuit2006.pdf:pdf},
issn = {0270-6474},
journal = {Journal of Neuroscience},
keywords = {Computational modeling,Dynamical systems,Intraparietal cortex,NMDA,Reaction time,Sensory discrimination},
mendeley-groups = {FYP},
number = {4},
pages = {1314--1328},
title = {{A recurrent network mechanism of time integration in perceptual decisions}},
volume = {26},
year = {2006}
}


@article{flachot_deep_2022,
	abstract = {Color constancy is our ability to perceive constant colors across varying illuminations. Here, we trained deep neural networks to be color constant and evaluated their performance with varying cues. Inputs to the networks consisted of two-dimensional images of simulated cone excitations derived from three-dimensional (3D) rendered scenes of 2,115 different 3D shapes, with spectral reflectances of 1,600 different Munsell chips, illuminated under 278 different natural illuminations. The models were trained to classify the reflectance of the objects. Testing was done with four new illuminations with equally spaced CIEL*a*b* chromaticities, two along the daylight locus and two orthogonal to it. High levels of color constancy were achieved with different deep neural networks, and constancy was higher along the daylight locus. When gradually removing cues from the scene, constancy decreased. Both ResNets and classical ConvNets of varying degrees of complexity performed well. However, DeepCC, our simplest sequential convolutional network, represented colors along the three color dimensions of human color vision, while ResNets showed a more complex representation.},
	author = {Flachot, Alban and Akbarinia, Arash and Sch{\"u}tt, Heiko H. and Fleming, Roland W. and Wichmann, Felix A. and Gegenfurtner, Karl R.},
	doi = {10.1167/jov.22.4.17},
	issn = {1534-7362},
	journal = {Journal of Vision},
	month = mar,
	note = {\_eprint: https://arvojournals.org/arvo/content\_public/journal/jov/938583/i1534-7362-22-4-17\_1648554492.00585.pdf},
	number = {4},
	pages = {17--17},
	title = {Deep neural models for color classification and color constancy},
	url = {https://doi.org/10.1167/jov.22.4.17},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1167/jov.22.4.17}}


@INPROCEEDINGS{linearregression,  author={Sari, Yuita Arum and Hari Ginardi, R. V. and Suciati, Nanik},  booktitle={2015 International Conference on Information & Communication Technology and Systems (ICTS)},   title={Color correction using improved linear regression algorithm},   year={2015},  volume={},  number={},  pages={73-78},  doi={10.1109/ICTS.2015.7379874}}

@ARTICLE{5719167,
  author={Gijsenij, Arjan and Gevers, Theo and van de Weijer, Joost},
  journal={IEEE Transactions on Image Processing}, 
  title={Computational Color Constancy: Survey and Experiments}, 
  year={2011},
  volume={20},
  number={9},
  pages={2475-2489},
  doi={10.1109/TIP.2011.2118224}}
  
@article{Land1971LightnessAR,
  title={Lightness and retinex theory.},
  author={Edwin Herbert Land and John J. McCann},
  journal={Journal of the Optical Society of America},
  year={1971},
  volume={61 1},
  pages={
          1-11
        }
}
@article{Geirhos_2020,
   title={Shortcut learning in deep neural networks},
   volume={2},
   ISSN={2522-5839},
   url={http://dx.doi.org/10.1038/s42256-020-00257-z},
   DOI={10.1038/s42256-020-00257-z},
   number={11},
   journal={Nature Machine Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
   year={2020},
   month={Nov},
   pages={665–673}
}
@article{Day1972TheBO,
  title={The basis of perceptual constancy and perceptual illusion.},
  author={R. Day},
  journal={Investigative ophthalmology},
  year={1972},
  volume={11 6},
  pages={
          525-32
        }
}

@inproceedings{hu2017fc,
	abstract = {Improvements in color constancy have arisen from the use of convolutional neural networks (CNNs). However, the patch-based CNNs that exist for this problem are faced with the issue of estimation ambiguity, where a patch may contain insufficient information to establish a unique or even a limited possible range of illumination colors. Image patches with estimation ambiguity not only appear with great frequency in photographs, but also significantly degrade the quality of network training and inference. To overcome this problem, we present a fully convolutional network architecture in which patches throughout an image can carry different confidence weights according to the value they provide for color constancy estimation. These confidence weights are learned and applied within a novel pooling layer where the local estimates are merged into a global solution. With this formulation, the network is able to determine "what to learn" and "how to pool" automatically from color constancy datasets without additional supervision. The proposed network also allows for end-to-end training, and achieves higher efficiency and accuracy. On standard benchmarks, our network outperforms the previous state-of-the-art while achieving 120x greater efficiency.},
	author = {Hu, Yuanming and Wang, Baoyuan and Lin, Steve},
	booktitle = {Computer Vision and Pattern Recognition (CVPR)},
	edition = {Computer Vision and Pattern Recognition (CVPR)},
	month = {July},
	title = {FC4: Fully Convolutional Color Constancy with Confidence-weighted Pooling},
	url = {https://www.microsoft.com/en-us/research/publication/fc4-fully-convolutional-color-constancy-confidence-weighted-pooling/},
	year = {2017},
	bdsk-url-1 = {https://www.microsoft.com/en-us/research/publication/fc4-fully-convolutional-color-constancy-confidence-weighted-pooling/}}


@article{sizeconstancymodel,
	abstract = {Size constancy is one of the well-known visual phenomena that demonstrates perceptual stability to account for the effect of viewing distance on retinal image size. Although theories involving distance scaling to achieve size constancy have flourished based on psychophysical studies, its underlying neural mechanisms remain unknown. Single cell recordings show that distance-dependent size tuned cells are common along the ventral stream, originating from V1, V2, and V4 leading to IT. In addition, recent research employing fMRI demonstrates that an object's perceived size, associated with its perceived egocentric distance, modulates its retinotopic representation in V1. These results suggest that V1 contributes to size constancy, and its activity is possibly regulated by feedback of distance information from other brain areas. Here, we propose a neural model based on these findings. First, we construct an egocentric distance map in LIP by integrating horizontal disparity and vergence through gain-modulated MT neurons. Second, LIP neurons send modulatory feedback of distance information to size tuned cells in V1, resulting in a spread of V1 cortical activity. This process provides V1 with distance-dependent size representations. The model supports that size constancy is preserved by scaling retinal image size to compensate for changes in perceived distance, and suggests a possible neural circuit capable of implementing this process.},
	author = {Qian, Jiehui AND Yazdanbakhsh, Arash},
	doi = {10.1371/journal.pone.0129377},
	journal = {PLOS ONE},
	month = {07},
	number = {7},
	pages = {1-19},
	publisher = {Public Library of Science},
	title = {A Neural Model of Distance-Dependent Percept of Object Size Constancy},
	url = {https://doi.org/10.1371/journal.pone.0129377},
	volume = {10},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pone.0129377}}
@techreport{Furukawa2017,
  title        = {Deep Learning for Target Classification from {SAR} Imagery: Data Augmentation and Translation Invariance},
  author       = {Hidetoshi Furukawa},
  institution  = {arXiv},
  number       = {1708.07920},
  type         = {Preprint},
  year         = {2017},
  month        = {August},
  url          = {https://arxiv.org/abs/1708.07920},
  note         = {arXiv:1708.07920 [cs.CV]},
}
@article{Rust2010selectivity,
  author    = {Nicole C. Rust and James J. DiCarlo},
  title     = {Selectivity and Tolerance (“Invariance”) Both Increase as Visual Information Propagates from Cortical Area V4 to IT},
  journal   = {Journal of Neuroscience},
  year      = {2010},
  volume    = {30},
  number    = {39},
  pages     = {12978--12995},
  doi       = {10.1523/JNEUROSCI.0179-10.2010},
  url       = {https://www.jneurosci.org/content/30/39/12978},
  issn      = {1529-2401},
  abstract  = {Recording across the ventral stream shows that neuronal responses become simultaneously more selective and more tolerant to position/size changes between V4 and IT, pointing to hierarchical mechanisms underlying invariant object recognition.}
}

@article{Ito1995size,
  author    = {Masao Ito, Edward K. Tamura and Keiji Fujita},
  title     = {Size and Position Invariance of Neuronal Responses in Monkey Inferotemporal Cortex},
  journal   = {Journal of Neurophysiology},
  year      = {1995},
  volume    = {73},
  number    = {1},
  pages     = {218--226},
  doi       = {10.1152/jn.1995.73.1.218},
  url       = {https://journals.physiology.org/doi/10.1152/jn.1995.73.1.218},
  issn      = {0022-3077},
  abstract  = {About one–fifth of anterior IT neurons maintained shape selectivity across large changes in retinal size and position, demonstrating intrinsic invariance mechanisms early in the ventral stream.}
}

@article{Logothetis1995shape,
  author    = {Nikolaus K. Logothetis and Jay Pauls and Tomaso Poggio},
  title     = {Shape Representation in the Inferior Temporal Cortex of Monkeys},
  journal   = {Current Biology},
  year      = {1995},
  volume    = {5},
  number    = {5},
  pages     = {552--563},
  doi       = {10.1016/S0960-9822(95)00108-4},
  url       = {https://doi.org/10.1016/S0960-9822(95)00108-4},
  issn      = {0960-9822},
  abstract  = {Systematic shape mapping in IT reveals neurons tuned to complex features that remain selective across changes in size, position, and cue type, supporting a neural code for invariant object recognition.}
}


@InProceedings{weiss2025,
author="Shifman, Ofir
and Weiss, Yair",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="Lost in Translation: Modern Neural Networks Still Struggle with Small Realistic Image Transformations",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="231--247",
abstract="Deep neural networks that achieve remarkable performance in image classification have previously been shown to be easily fooled by tiny transformations such as a one pixel translation of the input image. In order to address this problem, two approaches have been proposed in recent years. The first approach suggests using huge datasets together with data augmentation in the hope that a highly varied training set will teach the network to learn to be invariant. The second approach suggests using architectural modifications based on sampling theory to deal explicitly with image translations. In this paper, we show that these approaches still fall short in robustly handling `natural' image translations that simulate a subtle change in camera orientation. Our findings reveal that a mere one-pixel translation can result in a significant change in the predicted image representation for approximately 40{\%} of the test images in state-of-the-art models (e.g. open-CLIP trained on LAION-2B or DINO-v2) , while models that are explicitly constructed to be robust to cyclic translations can still be fooled with 1 pixel realistic (non-cyclic) translations 11{\%} of the time. We present Robust Inference by Crop Selection: a simple method that can be proven to achieve any desired level of consistency, although with a modest tradeoff with the model's accuracy. Importantly, we demonstrate how employing this method reduces the ability to fool state-of-the-art models with a 1 pixel translation to less than 5{\%} while suffering from only a 1{\%} drop in classification accuracy. Additionally, we show that our method can be easily adjusted to deal with circular shifts as well. In such a case we achieve 100{\%} robustness to integer shifts with state-of-the-art accuracy, and with no need for any further training. Code is available at: https://github.com/ofirshifman/RICS.",
isbn="978-3-031-72890-7"
}

@article{azulay2019why,
  author  = {Aharon Azulay and Yair Weiss},
  title   = {Why do deep convolutional networks generalize so poorly to small image transformations?},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {184},
  pages   = {1-25},
  url     = {http://jmlr.org/papers/v20/19-519.html}
}

@article{colorconstancyreview,
	abstract = {A quarter of a century ago, the first systematic behavioral experiments were performed to clarify the nature of color constancy---the effect whereby the perceived color of a surface remains constant despite changes in the spectrum of the illumination. At about the same time, new models of color constancy appeared, along with physiological data on cortical mechanisms and photographic colorimetric measurements of natural scenes. Since then, as this review shows, there have been many advances. The theoretical requirements for constancy have been better delineated and the range of experimental techniques has been greatly expanded; novel invariant properties of images and a variety of neural mechanisms have been identified; and increasing recognition has been given to the relevance of natural surfaces and scenes as laboratory stimuli. Even so, there remain many theoretical and experimental challenges, not least to develop an account of color constancy that goes beyond deterministic and relatively simple laboratory stimuli and instead deals with the intrinsically variable nature of surfaces and illuminations present in the natural world.},
	author = {David H. Foster},
	doi = {https://doi.org/10.1016/j.visres.2010.09.006},
	issn = {0042-6989},
	journal = {Vision Research},
	keywords = {Color constancy, Color appearance, Chromatic adaptation, Von Kries coefficient, Surface color, Asymmetric color matching, Color naming, Achromatic adjustment, Relational color constancy, Spatial ratios of cone excitations, Color-constancy indices, Illuminant estimation, Natural scene statistics, Spectral basis functions},
	note = {Vision Research 50th Anniversary Issue: Part 1},
	number = {7},
	pages = {674-700},
	title = {Color constancy},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698910004402},
	volume = {51},
	year = {2011},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0042698910004402},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.visres.2010.09.006}}




@incollection{VIQUEIRAPEREZ20103,
	abstract = {Abstract:
When viewing any scene, the human visual system is able to extract information regarding light wavelength, which is why we see in colour. This chapter discusses the mechanisms of human colour vision. The chapter first reviews the anatomy and the physiology of the visual system, and then describes the generic ATD models of colour vision. From these models, the chapter discusses the topics of colour appearance, colour constancy, and defective colour vision.},
	author = {V. {Viqueira P{\'e}rez} and D. {De Fez Saiz} and F. {Martinez Verd{\'u}}},
	booktitle = {Colour Measurement},
	doi = {https://doi.org/10.1533/9780857090195.1.3},
	editor = {M.L. Gulrajani},
	isbn = {978-1-84569-559-0},
	keywords = {ATD models, colour appearance, defective colour vision, colour constancy, mechanisms of chromatic adaptation},
	pages = {3-e2},
	publisher = {Woodhead Publishing},
	series = {Woodhead Publishing Series in Textiles},
	title = {1 - Colour vision: theories and principles},
	url = {https://www.sciencedirect.com/science/article/pii/B9781845695590500005},
	year = {2010},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/B9781845695590500005},
	Bdsk-Url-2 = {https://doi.org/10.1533/9780857090195.1.3}}


@article{10.3389/fnhum.2015.00190,
	abstract = {Although a visual illusion is often viewed as an amusing trick, for the vision scientist it is a question that demands an answer, which leads to even more questioning. All researchers hold their own chain of questions, the links of which depend on the very theory they adhere to. Perceptual theories are devoted to answering questions concerning sensation and perception, but in doing so they shape concepts such as reality and representation, which necessarily affect the concept of illusion. Here we consider the macroscopic aspects of such concepts in vision sciences from three classic viewpoints---Ecological, Cognitive, Gestalt approaches---as we see this a starting point to understand in which terms illusions can become a tool in the hand of the neuroscientist. In fact, illusions can be effective tools in studying the brain in reference to perception and also to cognition in a much broader sense. A theoretical debate is, however, mandatory, in particular with regards to concepts such as veridicality and representation. Whether a perceptual outcome is considered as veridical or illusory (and, consequently, whether a class of phenomena should be classified as perceptual illusions or not) depends on the meaning of such concepts.},
	author = {Zavagno, Daniele and Daneyko, Olga and Actis-Grosso, Rossana},
	doi = {10.3389/fnhum.2015.00190},
	issn = {1662-5161},
	journal = {Frontiers in Human Neuroscience},
	pages = {190},
	title = {Mishaps, errors, and cognitive experiences: on the conceptualization of perceptual illusions},
	url = {https://www.frontiersin.org/article/10.3389/fnhum.2015.00190},
	volume = {9},
	year = {2015},
	Bdsk-Url-1 = {https://www.frontiersin.org/article/10.3389/fnhum.2015.00190},
	Bdsk-Url-2 = {https://doi.org/10.3389/fnhum.2015.00190}}

@incollection{hoffman,

	author = {D. {Hoffman}},
	booktitle = {2005 McGraw-Hill yearbook of Science and technology},
	doi = {},
	editor = {McGraw-Hill},
	isbn = {},

	pages = {3-e2},
	publisher = {Department of Cognitive Science, University of California, Irvine},
	series = {},
	title = {Visual illusions and perception},
	url = {https://www.cogsci.uci.edu/~ddhoff/Mcgraw-Hill2005.pdf},
	year = {2005},
	Bdsk-Url-1 = {https://www.cogsci.uci.edu/~ddhoff/Mcgraw-Hill2005.pdf},
}

@article{classification,
	annote = {doi: 10.1016/S1364-6613(97)01060-7},
	author = {Gregory, Richard L. },
	booktitle = {Trends in Cognitive Sciences},
	date = {1997/08/01},
	date-added = {2021-09-28 22:05:30 -0400},
	date-modified = {2021-09-28 22:05:30 -0400},
	doi = {10.1016/S1364-6613(97)01060-7},
	isbn = {1364-6613},
	journal = {Trends in Cognitive Sciences},
	m3 = {doi: 10.1016/S1364-6613(97)01060-7},
	month = {2021/09/28},
	number = {5},
	pages = {190--194},
	publisher = {Elsevier},
	title = {Visual illusions classified},
	ty = {JOUR},
	url = {https://doi.org/10.1016/S1364-6613(97)01060-7},
	volume = {1},
	year = {1997},
	year1 = {1997},
	Bdsk-Url-1 = {https://doi.org/10.1016/S1364-6613(97)01060-7}}


@article{HOFFMAN196665,
	abstract = {The familiar perceptual constancies of image location in the field of view, image orientation, size constancy, shape constancy, binocular distortion, and motion, have their natural mathematical expression in terms of Lie groups of transformations over the visual manifold. If Lie's three fundamental theorems are to be satisfied, three additional perceptual invariances must also be present: time, efferent binocularity, and what apparently constitutes some sort of circulating memory in space-time. This Lie algebra of visual perception admits ready explanations for the following visual phenomena: the developmental sequence of infant vision; orthogonal after-images; after-effects of seen movement; the spiral after-effect and the spiral images sometimes evoked under flicker; reading reversals; and the visual analogue of the Fitzgerald contration. The theory also predicts certain new complementary (orthogonal) after-images, the existence of which have been verified experimentally.},
	author = {William C. Hoffman},
	doi = {https://doi.org/10.1016/0022-2496(66)90005-8},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	number = {1},
	pages = {65-98},
	title = {The Lie algebra of visual perception},
	url = {https://www.sciencedirect.com/science/article/pii/0022249666900058},
	volume = {3},
	year = {1966},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/0022249666900058},
	Bdsk-Url-2 = {https://doi.org/10.1016/0022-2496(66)90005-8}}

@article{learningliegroups,
    author = {Rao, Rajesh and Ruderman, Dan},
year = {1999},
month = {01},
pages = {},
title = {Learning Lie Groups for Invariant Visual Perception},
volume = {11},
journal = {Advances in Neural Information Processing Systems}
}
@book{visionscience,
  added-at = {2010-03-01T21:54:28.000+0100},
  address = {Cambridge, Mass.},
  author = {Palmer, Stephen E.},
  biburl = {https://www.bibsonomy.org/bibtex/29ad1bd9c4643ce0bf2db15816c33e265/barabas},
  date-added = {2008-09-19 19:04:09 -0400},
  date-modified = {2008-09-19 19:04:09 -0400},
  description = {References for general exam},
  interhash = {83b8a1a732b0b935d28fbf5960da58a6},
  intrahash = {9ad1bd9c4643ce0bf2db15816c33e265},
  isbn = {0262161834 Thanks for using Barton, the MIT Libraries' catalog http},
  keywords = {Cognitive Vision; Visual perception; science},
  note = {Stephen E. Palmer.; "A Bradford book."; Bibliography: Includes bibliographical references (p. [737]-769) and indexes.},
  pages = 810,
  publisher = {MIT Press},
  timestamp = {2010-03-01T21:54:30.000+0100},
  title = {Vision science : photons to phenomenology},
  year = 1999
}



@article{10.3389/fncom.2021.681162,
	abstract = {Perceptual constancy refers to the fact that the perceived geometrical and physical characteristics of objects remain constant despite transformations of the objects such as rigid motion. Perceptual constancy is essential in everything we do, like recognition of familiar objects and scenes, planning and executing visual navigation, visuomotor coordination, and many more. Perceptual constancy would not exist without the geometrical and physical permanence of objects: their shape, size, and weight. Formally, perceptual constancy and permanence of objects are invariants, also known in mathematics and physics as symmetries. Symmetries of the Laws of Physics received a central status due to mathematical theorems of Emmy Noether formulated and proved over 100 years ago. These theorems connected symmetries of the physical laws to conservation laws through the least-action principle. We show how Noether's theorem is applied to mirror-symmetrical objects and establishes mental shape representation (perceptual conservation) through the application of a simplicity (least-action) principle. This way, the formalism of Noether's theorem provides a computational explanation of the relation between the physical world and its mental representation.},
	author = {Pizlo, Zygmunt and de Barros, J. Acacio},
	doi = {10.3389/fncom.2021.681162},
	issn = {1662-5188},
	journal = {Frontiers in Computational Neuroscience},
	pages = {73},
	title = {The Concept of Symmetry and the Theory of Perception},
	url = {https://www.frontiersin.org/article/10.3389/fncom.2021.681162},
	volume = {15},
	year = {2021},
	Bdsk-Url-1 = {https://www.frontiersin.org/article/10.3389/fncom.2021.681162},
	Bdsk-Url-2 = {https://doi.org/10.3389/fncom.2021.681162}}
@article{DBLP:journals/corr/abs-2104-13478,
  author    = {Michael M. Bronstein and
               Joan Bruna and
               Taco Cohen and
               Petar Velickovic},
  title     = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  journal   = {CoRR},
  volume    = {abs/2104.13478},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.13478},
  eprinttype = {arXiv},
  eprint    = {2104.13478},
  timestamp = {Tue, 04 May 2021 15:12:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-13478.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{PHILLIPS20151,
	abstract = {A broad neuron-centric conception of contextual modulation is reviewed and re-assessed in the light of recent neurobiological studies of amplification, suppression, and synchronization. Behavioural and computational studies of perceptual and higher cognitive functions that depend on these processes are outlined, and evidence that those functions and their neuronal mechanisms are impaired in schizophrenia is summarized. Finally, we compare and assess the long-term biological functions of contextual modulation at the level of computational theory as formalized by the theories of coherent infomax and free energy reduction. We conclude that those theories, together with the many empirical findings reviewed, show how contextual modulation at the neuronal level enables the cortex to flexibly adapt the use of its knowledge to current circumstances by amplifying and grouping relevant activities and by suppressing irrelevant activities.},
	author = {W.A. Phillips and A. Clark and S.M. Silverstein},
	doi = {https://doi.org/10.1016/j.neubiorev.2015.02.010},
	issn = {0149-7634},
	journal = {Neuroscience & Biobehavioral Reviews},
	keywords = {Cognitive coordination, Contextual modulation, Amplification, Suppression, Gestalt grouping, Predictive processing},
	pages = {1-20},
	title = {On the functions, mechanisms, and malfunctions of intracortical contextual modulation},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763415000573},
	volume = {52},
	year = {2015},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0149763415000573},
	bdsk-url-2 = {https://doi.org/10.1016/j.neubiorev.2015.02.010}}

@article{Mly2018ComplementarySE,
  title={Complementary Surrounds Explain Diverse Contextual Phenomena Across Visual Modalities},
  author={David A. M{\'e}ly and Drew Linsley and Thomas Serre},
  journal={Psychological Review},
  year={2018},
  volume={125},
  pages={769–784}
}


@article{walsh_how_1999,
	author = {Walsh, V},
	doi = {10.1073/pnas.96.24.13594},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	keywords = {Cerebral Cortex/*physiology, Color Perception/*physiology, Humans, Magnetic Resonance Imaging, Occipital Lobe/physiology, Parietal Lobe/physiology},
	language = {eng},
	month = nov,
	note = {Publisher: National Academy of Sciences},
	number = {24},
	pages = {13594--13596},
	title = {How does the cortex construct color?},
	url = {https://pubmed.ncbi.nlm.nih.gov/10570116},
	volume = {96},
	year = {1999},
	bdsk-url-1 = {https://pubmed.ncbi.nlm.nih.gov/10570116},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.96.24.13594}}
	@ARTICLE{Ruderman98statisticsof,
    author = {Daniel L. Ruderman and Thomas W. Cronin and Chuan-Chin Chiao},
    title = {Statistics of Cone Responses to Natural Images: Implications for Visual Coding},
    journal = {Journal of the Optical Society of America A},
    year = {1998},
    volume = {15},
    pages = {2036--2045}
}
@article{v4columns,
author = {Westerberg, Jacob and Schall, Jeffrey and Maier, Alexander},
year = {2020},
month = {01},
pages = {},
title = {Color Columns in Visual Area V4},
journal = {SSRN Electronic Journal},
doi = {10.2139/ssrn.3596607}
}

@misc{schuhmann2022laion5b,
      title={LAION-5B: An open large-scale dataset for training next generation image-text models}, 
      author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
      year={2022},
      eprint={2210.08402},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@ARTICLE{Fel2022-dt,
  title   = "Harmonizing the object recognition strategies of deep neural
             networks with humans",
  author  = "Fel*, Thomas and Rodriguez*, Ivan F and Linsley*, Drew and Serre,
             Thomas",
  journal = "Adv. Neural Inf. Process. Syst.",
  year    =  2022
}

@article{Benjamin780478,
	abstract = {Neurons are often probed by presenting a set of stimuli that vary along one dimension (e.g. color) and quantifying how this stimulus property affect neural activity. An open question, in particular where higher-level areas are involved, is how much tuning measured with one stimulus set reveals about tuning to a new set. Here we ask this question by estimating tuning to hue in macaque V4 from a set of natural scenes and a set of simple color stimuli. We found that hue tuning was strong in each dataset but was not correlated across the datasets, a finding expected if neurons have strong mixed selectivity. We also show how such mixed selectivity may be useful for transmitting information about multiple dimensions of the world. Our finding suggest that tuning in higher visual areas measured with simple stimuli may thus not generalize to naturalistic stimuli.New \&amp; Noteworthy Visual cortex is often investigated by mapping neural tuning to variables selected by the researcher such as color. How much does this approach tell us a neuron{\textquoteright}s general {\textquoteleft}role{\textquoteright} in vision? Here we show that for strongly hue-tuned neurons in V4, estimating hue tuning from artificial stimuli does not reveal the hue tuning in the context of natural scenes. We show how models of optimal information processing suggest that such mixed selectivity maximizes information transmission.Competing Interest StatementThe authors have declared no competing interest.},
	author = {Benjamin, Ari S. and Ramkumar, Pavan and Fernandes, Hugo and Smith, Matthew and Kording, Konrad P.},
	doi = {10.1101/780478},
	elocation-id = {780478},
	eprint = {https://www.biorxiv.org/content/early/2022/01/09/780478.full.pdf},
	journal = {bioRxiv},
	publisher = {Cold Spring Harbor Laboratory},
	title = {Hue tuning curves in V4 change with visual context},
	url = {https://www.biorxiv.org/content/early/2022/01/09/780478},
	year = {2022},
	bdsk-url-1 = {https://www.biorxiv.org/content/early/2022/01/09/780478},
	bdsk-url-2 = {https://doi.org/10.1101/780478}}

@article{angular_loss,
  author={Hordley, S.D. and Finlayson, G.D.},
  booktitle={Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.}, 
  title={Re-evaluating colour constancy algorithms}, 
  year={2004},
  volume={1},
  number={1},
  pages={76-79 Vol.1},
  doi={10.1109/ICPR.2004.1334009}}

@article{ZAIDI2019169,
	abstract = {How we see colors is a great mystery, but also a route to understanding how we experience any quale, because color does not exist in the world outside our brains, and is undetectable by other senses. From photoreception to primary visual cortex, the neural encoding and transmission of color signals is well understood, providing a foundation for understanding cortical computations of color appearance. We describe how probabilistic models coupled with fMRI-guided microelectrode recordings from inferior-temporal macaque cortex (IT) could help us understand color decoding: i.e. how appearance is extracted from the neuronal responses evoked by a stimulus. Neurons in IT respond to a narrow range of colors with their peak responses scattered around the color circle. We discuss how intra-cellular processes and cortical circuits could generate such tuning curves, and how they approximate optimal Bayesian decoders in winner-take-all schemes.},
	author = {Qasim Zaidi and Bevil Conway},
	doi = {https://doi.org/10.1016/j.cobeha.2019.10.011},
	issn = {2352-1546},
	journal = {Current Opinion in Behavioral Sciences},
	note = {Visual perception},
	pages = {169-177},
	title = {Steps towards neural decoding of colors},
	url = {https://www.sciencedirect.com/science/article/pii/S2352154619300841},
	volume = {30},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S2352154619300841},
	bdsk-url-2 = {https://doi.org/10.1016/j.cobeha.2019.10.011}}

@article{colortunned,
abstract = {Large islands of extrastriate cortex that are enriched for color-tuned neurons have recently been described in alert macaque using a combination of functional magnetic resonance imaging (fMRI) and single-unit recording. These millimeter-sized islands, dubbed "globs," are scattered throughout the posterior inferior temporal cortex (PIT), a swath of brain anterior to area V3, including areas V4, PITd, and posterior TEO. We investigated the micro-organization of neurons within the globs. We used fMRI to identify the globs and then used MRI-guided microelectrodes to test the color properties of single glob cells. We used color stimuli that sample the CIELUV perceptual color space at regular intervals to test the color tuning of single units, and make two observations. First, color-tuned neurons of various color preferences were found within single globs. Second, adjacent glob cells tended to have the same color tuning, demonstrating that glob cells are clustered by color preference and suggesting that they are arranged in color columns. Neurons separated by 50 $\mu$m, measured parallel to the cortical sheet, had more similar color tuning than neurons separated by 100 $\mu$m, suggesting that the scale of the color columns is \<100 $\mu$m. These results show that color-tuned neurons in PIT are organized by color preference on a finer scale than the scale of single globs. Moreover, the color preferences of neurons recorded sequentially along a given electrode penetration shifted gradually in many penetrations, suggesting that the color columns are arranged according to a chromotopic map reflecting perceptual color space.},
author = {Conway, Bevil R and Tsao, Doris Y},
doi = {10.1073/pnas.0810943106},
journal = {Proceedings of the National Academy of Sciences},
number = {42},
pages = {18034--18039},
title = {{Color-tuned neurons are spatially clustered according to color preference within alert macaque posterior inferior temporal cortex}},
url = {https://www.pnas.org/doi/abs/10.1073/pnas.0810943106},
volume = {106},
year = {2009}
}
@inproceedings{Oord2018CPC,
  author       = {Aaron van den Oord and Yazhe Li and Oriol Vinyals},
  title        = {Representation Learning with Contrastive Predictive Coding},
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  year         = {2018},
  pages        = {10293--10304},
  url          = {https://arxiv.org/abs/1807.03748},
  note         = {arXiv:1807.03748},
  abstract     = {Introduces Contrastive Predictive Coding (CPC), a self-supervised method that learns powerful latent representations by predicting future observations in latent space using an InfoNCE contrastive loss.}
}

@inproceedings{Chen2020SimCLR,
  author       = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  title        = {A Simple Framework for Contrastive Learning of Visual Representations},
  booktitle    = {International Conference on Machine Learning (ICML)},
  year         = {2020},
  pages        = {1597--1607},
  url          = {https://arxiv.org/abs/2002.05709},
  note         = {arXiv:2002.05709},
  abstract     = {Presents SimCLR, a minimal yet effective contrastive learning framework that, with large batch sizes and extensive data augmentation, matches or surpasses supervised ImageNet pre-training on a range of downstream tasks.}
}


@article{Ringach2002Spatial,
  author    = {Dario L. Ringach},
  title     = {Spatial Structure and Symmetry of Simple-Cell Receptive Fields in Macaque Primary Visual Cortex},
  journal   = {Journal of Neurophysiology},
  year      = {2002},
  volume    = {88},
  number    = {1},
  pages     = {455--463},
  doi       = {10.1152/jn.2002.88.1.455},
  url       = {https://journals.physiology.org/doi/abs/10.1152/jn.2002.88.1.455},
  issn      = {0022-3077},
  abstract  = {Quantitative mapping shows that macaque simple cells are well described by 2‑D Gabor functions whose spatial profiles cluster into even‑ and odd‑symmetric classes, providing empirical constraints on models of V1 receptive‑field organization.}
}


@article{lindeberg2021normative,
  author    = {Lindeberg, Tony},
  title     = {Normative Theory of Visual Receptive Fields},
  journal   = {Heliyon},
  year      = {2021},
  volume    = {7},
  number    = {1},
  pages     = {e05897},
  doi       = {10.1016/j.heliyon.2021.e05897},
  url       = {https://doi.org/10.1016/j.heliyon.2021.e05897},
  issn      = {2405-8440},
  abstract  = {This article presents an axiomatic, scale‑space–based framework that derives idealized spatial, spatio‑chromatic, and spatio‑temporal receptive‑field families from symmetry and covariance requirements, linking early visual processing to principled computational constraints.}
}


@article{v4monkeys,
	abstract = { Cortical area V4 in monkeys contains neurons that respond selectively to particular colors. It has been controversial how these color-selective neurons are spatially organized in V4. One view asserts that color-selective neurons are organized in columns with different colors orderly mapped across the cortex, whereas other studies have found no evidence for columnar organization or any other clustered structure. In the present study, we reexamined the functional organization of color-selective neurons in area V4 by quantitatively evaluating and comparing the color selectivity of nearby neurons as well as those encountered along electrode penetrations. Using a multiple single-unit recording technique, we recorded extracellular activities simultaneously from groups of nearby V4 neurons. Color discrimination and color preferences exhibited a moderate correlation between nearby neurons, consistent with neurons in a local region of V4 sharing similar responses to stimulus color. However, the degree of clustering was variable across recording sites. Some regions contained neurons with similar color preferences, whereas others contained neurons with diverse color preferences. Neurons in penetrations normal to the cortical surface responded to an overlapping range of colors and maintained a moderate correlation. Neurons in penetrations tangential to the cortical surface differed dramatically in their preferred color and exhibited a negative correlation. We conclude that neurons in area V4 are moderately clustered according to their color selectivity and that this weak clustering is columnar in structure. },
	author = {Kotake, Yasuyo and Morimoto, Hiroshi and Okazaki, Yasutaka and Fujita, Ichiro and Tamura, Hiroshi},
	doi = {10.1152/jn.90624.2008},
	eprint = {https://doi.org/10.1152/jn.90624.2008},
	journal = {Journal of Neurophysiology},
	note = {PMID: 19369361},
	number = {1},
	pages = {15-27},
	title = {Organization of Color-Selective Neurons in Macaque Visual Area V4},
	url = {https://doi.org/10.1152/jn.90624.2008},
	volume = {102},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1152/jn.90624.2008}}

@article{zeki_neurological_1999,
	abstract = {We have studied patient PB, who, after an electric shock that led to vascular insufficiency, became virtually blind, although he retained a capacity to see colors consciously. For our psychophysical studies, we used a simplified version of the Land experiments [Land, E. (1974) Proc. R. Inst. G. B. 47, 23-58] to learn whether color constancy mechanisms are intact in him, which amounts to learning whether he can assign a constant color to a surface in spite of changes in the precise wavelength composition of the light reflected from that surface. We supplemented our psychophysical studies with imaging ones, using functional magnetic resonance, to learn something about the location of areas that are active in his brain when he perceives colors. The psychophysical results suggested that color constancy mechanisms are severely defective in PB and that his color vision is wavelength-based. The imaging results showed that, when he viewed and recognized colors, significant increases in activity were restricted mainly to V1-V2. We conclude that a partly defective color system operating on its own in a severely damaged brain is able to mediate a conscious experience of color in the virtually total absence of other visual abilities.},
	author = {Zeki, S and Aglioti, S and McKeefry, D and Berlucchi, G},
	doi = {10.1073/pnas.96.24.14124},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	keywords = {*Color Perception, Blindness/diagnosis/*physiopathology/psychology, Brain/pathology/*physiopathology, Humans, Magnetic Resonance Imaging, Male},
	language = {eng},
	month = nov,
	note = {Publisher: National Academy of Sciences},
	number = {24},
	pages = {14124--14129},
	title = {The neurological basis of conscious color perception in a blind patient},
	url = {https://pubmed.ncbi.nlm.nih.gov/10570209},
	volume = {96},
	year = {1999},
	bdsk-url-1 = {https://pubmed.ncbi.nlm.nih.gov/10570209},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.96.24.14124}}


@INPROCEEDINGS{cinco,
  author={Bleier, Michael and Riess, Christian and Beigpour, Shida and Eibenberger, Eva and Angelopoulou, Elli and Tröger, Tobias and Kaup, André},
  booktitle={2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)}, 
  title={Color constancy and non-uniform illumination: Can existing algorithms work?}, 
  year={2011},
  volume={},
  number={},
  pages={774-781},
  doi={10.1109/ICCVW.2011.6130331}}
@article{Riesenhuber1999,
  author = {Riesenhuber, Maximilian and Poggio, Tomaso},
  title = {Hierarchical models of object recognition in cortex},
  journal = {Nature Neuroscience},
  volume = {2},
  pages = {1019--1025},
  year = {1999}
}

@phdthesis{Serre2006Thesis,
  author = {Serre, Thomas},
  title = {Learning a Dictionary of Shape-Components in Visual Cortex},
  school = {Massachusetts Institute of Technology},
year = {2006}
}

@article{OlshausenField1996,
  author = {Olshausen, Bruno A. and Field, David J.},
  title = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  journal = {Nature},
  volume = {381},
  pages = {607--609},
  year = {1996}
}

@article{LeeMumford2003,
  author = {Lee, Tai Sing and Mumford, David},
  title = {Hierarchical Bayesian inference in the visual cortex},
  journal = {Journal of the Optical Society of America A},
  volume = {20},
	number = {7},
  pages = {1434--1448},
  year = {2003}
}

@article{MasquelierThorpe2007,
  author = {Masquelier, Timothée and Thorpe, Simon J.},
  title = {Unsupervised learning of visual features through spike-timing-dependent plasticity},
  journal = {PLoS Computational Biology},
	volume = {3},
  number = {2},
  pages = {e31},
  year = {2007}
}

@inproceedings{MutchLowe2008,
  author = {Mutch, Jim and Lowe, David G.},
  title = {Object class recognition using transferable feature representations},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2008}
}

@article{RiesenhuberPoggio1999,
  author  = {Riesenhuber, Maximilian and Poggio, Tomaso},
  title   = {Hierarchical models of object recognition in cortex},
  journal = {Nature Neuroscience},
  volume  = {2},
  number  = {11},
  pages   = {1019--1025},
  year    = {1999}
}

@article{Lampl2004,
  author  = {Lampl, Ilan and Ferster, David and Poggio, Tomaso and Riesenhuber, Maximilian},
  title   = {Intracellular measurements of spatial integration and the max operation in complex cells of the cat primary visual cortex},
  journal = {Journal of Neurophysiology},
  volume  = {92},
  number  = {5},
  pages   = {2704--2713},
  year    = {2004}
}

@article{Crick1989,
  author  = {Crick, Francis},
  title   = {The recent excitement about neural networks},
  journal = {Nature},
  volume  = {337},
  pages   = {129--132},
  year    = {1989}
}

@article{Hinton2006,
  author  = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  title   = {A fast learning algorithm for deep belief nets},
  journal = {Neural Computation},
  volume  = {18},
  number  = {7},
  pages   = {1527--1554},
  year    = {2006}
}

@inproceedings{Serre2005cvpr,
  author = {Serre, Thomas and Wolf, Lior and Bileschi, Stanley and Riesenhuber, Maximilian and Poggio, Tomaso},
  title = {Object recognition with features inspired by visual cortex},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2005}
}

@article{Serre2007pami,
  author = {Serre, Thomas and Wolf, Lior and Poggio, Tomaso},
  title = {Object recognition with features inspired by visual cortex},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {29},
  number = {3},
  pages = {411--426},
  year = {2007}
}

@article{Serre2007pnas,
  author  = {Serre, Thomas and Oliva, Aude and Poggio, Tomaso},
  title   = {A feedforward architecture accounts for rapid categorization},
	journal = {Proceedings of the National Academy of Sciences},
  volume  = {104},
  pages   = {6424--6429},
  year    = {2007}
}

@article{HubelWiesel1962,
  author = {Hubel, David H. and Wiesel, Torsten N.},
  title = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
  journal = {Journal of Physiology},
  volume = {160},
  pages = {106--154},
  year = {1962}
}

@article{PerrettOram1993,
  author  = {Perrett, David I. and Oram, Michael W.},
  title   = {Neurophysiology of Shape Processing},
  journal = {Image and Vision Computing},
  volume  = {11},
  number  = {6},
  pages   = {317--333},
  year    = {1993},
  doi     = {10.1016/0262-8856(93)90011-5}
}

@article{WallisRolls1997,
  author  = {Wallis, Guy and Rolls, Edmund T.},
  title   = {Invariant Face and Object Recognition in the Visual System},
  journal = {Progress in Neurobiology},
  volume  = {51},
  number  = {2},
  pages   = {167--194},
  year    = {1997},
  doi     = {10.1016/S0301-0082(96)00054-8}
}

@book{Rolls2012,
  author    = {Rolls, Edmund T.},
  title     = {Invariant Visual Object Recognition in the Visual System},
  publisher = {MIT Press},
  year      = {2012}
}

@article{Logothetis1995,
  author  = {Logothetis, Nikos K. and Pauls, Jon G. and B{\"u}lthoff, Heinrich H. and Poggio, Tomaso},
  title   = {View-dependent object recognition by monkeys},
  journal = {Current Biology},
  volume  = {5},
  number  = {5},
  pages   = {552--563},
  year    = {1995}
}

@article{DiCarlo2012,
  author  = {DiCarlo, James J. and Zoccolan, Davide and Rust, Nicole C.},
  title   = {How does the brain solve visual object recognition?},
  journal = {Neuron},
  volume  = {73},
  number  = {3},
  pages   = {415--434},
  year    = {2012}
}

@article{Serre2019GoodBadUgly,
  title   = {Deep Learning: The Good, the Bad, and the Ugly},
  author  = {Serre, Thomas},
  journal = {Annual Review of Vision Science},
  year    = {2019},
  volume  = {5},
  pages   = {399--426},
  doi     = {10.1146/annurev-vision-091718-014951}
}

@misc{LinsleyFengSerreTICS,
  author       = {Linsley, Drew and Feng, Pinyuan and Serre, Thomas},
  title        = {Better artificial intelligence does not mean better models of biology},
  note         = {In press at \emph{Trends in Cognitive Sciences}},
  year         = {2025},
  eprint       = {2504.16940},
  archivePrefix= {arXiv},
  primaryClass = {q-bio.NC}
}

@article{Yamins2014,
  author  = {Yamins, Daniel L. K. and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
  title   = {Performance-optimized hierarchical models predict neural responses in higher visual cortex},
  journal = {Proceedings of the National Academy of Sciences},
  volume  = {111},
  number  = {23},
  pages   = {8619--8624},
  year    = {2014}
}

@article{KhalighRazavi2014,
  author  = {Khaligh-Razavi, Seyed-Mahdi and Kriegeskorte, Nikolaus},
  title   = {Deep supervised, but not unsupervised, models may explain IT cortical representation},
  journal = {PLoS Computational Biology},
  volume  = {10},
  number  = {11},
  pages   = {e1003915},
  year    = {2014}
}

@article{Cadieu2014,
  author  = {Cadieu, Charles F. and Hong, Ha and Yamins, Daniel L. K. and Pinto, Nicolas and Ardila, Diego and Solomon, Ethan A. and DiCarlo, James J.},
  title   = {Deep neural networks rival the representation of primate IT cortex for core visual object recognition},
  journal = {PLoS Computational Biology},
  volume  = {10},
  number  = {12},
  pages   = {e1003963},
  year    = {2014}
}

@article{Foldiak1991,
  author  = {F{\"o}ldi{\'a}k, Peter},
  title   = {Learning invariance from transformation sequences},
  journal = {Neural Computation},
  volume  = {3},
  number  = {2},
  pages   = {194--200},
  year    = {1991}
}

@article{PoggioEdelman1990,
  author  = {Poggio, Tomaso and Edelman, Shimon},
  title   = {A network that learns to recognize three-dimensional objects},
  journal = {Nature},
  volume  = {343},
  pages   = {263--266},
  year    = {1990}
}

@article{Tanaka1996,
  author  = {Tanaka, Keiji},
  title   = {Inferotemporal Cortex and Object Vision},
  journal = {Annual Review of Neuroscience},
  volume  = {19},
  pages   = {109--139},
  year    = {1996},
  doi     = {10.1146/annurev.ne.19.030196.000545}
}

@article{LogothetisSheinberg1996,
  author  = {Logothetis, Nikos K. and Sheinberg, David L.},
  title   = {Visual Object Recognition},
  journal = {Annual Review of Neuroscience},
  volume  = {19},
  pages   = {577--621},
  year    = {1996},
  doi     = {10.1146/annurev.ne.19.030196.003045}
}

@article{Thorpe1996,
  author  = {Thorpe, Simon J. and Fize, Delphine and Marlot, Catherine},
  title   = {Speed of processing in the human visual system},
  journal = {Nature},
  volume  = {381},
  pages   = {520--522},
  year    = {1996}
}

@article{Krizhevsky2012,
  author  = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title   = {ImageNet classification with deep convolutional neural networks},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {25},
  year    = {2012}
}

@article{Russakovsky2015,
  author  = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and others},
  title   = {ImageNet Large Scale Visual Recognition Challenge},
  journal = {International Journal of Computer Vision},
  volume  = {115},
  number  = {3},
  pages   = {211--252},
  year    = {2015}
}

@article{Szegedy2015,
  author  = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and others},
  title   = {Going deeper with convolutions},
  journal = {IEEE Conference on Computer Vision and Pattern Recognition},
  year    = {2015}
}

@article{He2016,
  author  = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title   = {Deep residual learning for image recognition},
  journal = {IEEE Conference on Computer Vision and Pattern Recognition},
  year    = {2016}
}

@article{SerrePavlickNeuron2025,
  title={From Prediction to Understanding: Will AI Foundation Models Transform Brain Science?},
  author={Serre, Thomas and Pavlick, Ellie},
  journal={Neuron},
  year={2025},
  note={Perspective/NeuroView}
}

@article{FellemanVanEssen1991,
  title={Distributed hierarchical processing in the primate cerebral cortex},
  author={Felleman, Daniel J. and Van Essen, David C.},
  journal={Cerebral Cortex},
  volume={1},
  number={1},
  pages={1--47},
  year={1991}
}

@article{Fukushima1980,
  title={Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  author={Fukushima, Kunihiko},
  journal={Biological Cybernetics},
  volume={36},
  number={4},
  pages={193--202},
  year={1980}
}

@misc{Goodfellow2009,
  title={Measuring Invariances in Deep Networks},
  author={Goodfellow, Ian J. and Le, Quoc V. and Saxe, Andrew and Ng, Andrew Y.},
  booktitle={Advances in Neural Information Processing Systems},
  year={2009}
}

@article{BiscioneBowers2021,
  title={Convolutional Neural Networks Are Not Invariant to Translation, But Can Learn To Be},
  author={Biscione, Vittorio and Bowers, Jeffrey S.},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={45},
  pages={1--24},
  year={2021}
}

@article{Blything2021,
  title={The human visual system and CNNs can both support robust online translation tolerance following extreme displacements},
  author={Blything, Ryan P. and Baker, Daniel H. and Meese, Tim S.},
  journal={Journal of Vision},
  volume={21},
  number={10},
  pages={1--19},
  year={2021}
}

@article{Cui2020,
  title={Study on Representation Invariances of CNNs and Human Vision},
  author={Cui, Yin and Wang, Qian and Belongie, Serge},
  journal={Neurocomputing},
  volume={398},
  pages={371--381},
  year={2020}
}

@article{Graziani2021,
  title={On the Scale Invariance in State of the Art CNNs Trained on ImageNet},
  author={Graziani, Mara and Andrearczyk, Vincent and Marchand-Maillet, Stéphane},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021}
}

@article{LehkyTanaka2016,
  title   = {Neural Representation for Object Recognition in Inferotemporal Cortex},
  author  = {Lehky, Sidney R. and Tanaka, Keiji},
  journal = {Current Opinion in Neurobiology},
  year    = {2016},
  volume  = {37},
  pages   = {23--35},
  doi     = {10.1016/j.conb.2015.12.001}
}

@article{Rolls2000,
  title   = {Functions of the Primate Temporal Lobe Cortical Visual Areas in Invariant Visual Object and Face Recognition},
  author  = {Rolls, Edmund T.},
  journal = {Neuron},
  year    = {2000},
  volume  = {27},
  number  = {2},
  pages   = {205--218},
  doi     = {10.1016/S0896-6273(00)00030-1}
}

@article{RollsMilward2000,
  title   = {A Model of Invariant Object Recognition in the Visual System: Learning Rules, Activation Functions, Lateral Inhibition, and Information-Based Performance Measures},
  author  = {Rolls, Edmund T. and Milward, T.},
  journal = {Neural Computation},
  year    = {2000},
  volume  = {12},
  number  = {11},
  pages   = {2547--2572},
  doi     = {10.1162/089976600300014845}
}

@incollection{RicciSerre2020,
  title     = {Hierarchical Models of the Visual System},
  author    = {Ricci, Matthew and Serre, Thomas},
  booktitle = {Encyclopedia of Computational Neuroscience},
  editor    = {Jaeger, Dieter and Jung, Ranu},
  publisher = {Springer},
  address   = {New York, NY},
  year      = {2020},
  pages     = {1--14},
  doi       = {10.1007/978-1-4614-7320-6_345-2}
}

@incollection{LindsaySerre2021,
  title     = {Deep Learning Networks and Visual Perception},
  author    = {Lindsay, Grace W. and Serre, Thomas},
  booktitle = {Oxford Research Encyclopedia of Psychology},
  publisher = {Oxford University Press},
  year      = {2021},
  doi       = {10.1093/acrefore/9780190236557.013.841}
}

@article{HubelWiesel1977,
  title   = {Ferrier lecture. Functional architecture of macaque monkey visual cortex},
  author  = {Hubel, David H. and Wiesel, Torsten N.},
  journal = {Proceedings of the Royal Society of London. Series B, Biological Sciences},
  year    = {1977},
  volume  = {198},
  number  = {1130},
  pages   = {1--59},
  doi     = {10.1098/rspb.1977.0085}
}

@article{Tootell1988SF,
  title   = {Functional anatomy of macaque striate cortex. V. Spatial frequency},
  author  = {Tootell, Roger B. H. and Silverman, Michael S. and Hamilton, Susan L. and Switkes, Eliot and De Valois, Russell L.},
  journal = {Journal of Neuroscience},
  year    = {1988},
  volume  = {8},
  number  = {5},
  pages   = {1610--1624},
  doi     = {10.1523/JNEUROSCI.08-05-01610.1988}
}

@article{Nauhaus2012,
  title   = {Orthogonal micro-organization of orientation and spatial frequency in primate primary visual cortex},
  author  = {Nauhaus, Ian and Nielsen, Kristina J. and Disney, Anita A. and Callaway, Edward M.},
  journal = {Nature Neuroscience},
  year    = {2012},
  volume  = {15},
  number  = {12},
  pages   = {1683--1690},
  doi     = {10.1038/nn.3255}
}

@article{LuRoe2007,
  title   = {Optical Imaging of Contrast Response in Macaque Monkey V1 and V2},
  author  = {Lu, Haidong D. and Roe, Anna Wang},
  journal = {Cerebral Cortex},
  year    = {2007},
  volume  = {17},
  number  = {11},
  pages   = {2675--2695},
  doi     = {10.1093/cercor/bhl177}
}

@article{Zhang2023SFV2V4,
  title   = {Spatial frequency representation in V2 and V4 of macaque monkey},
  author  = {Zhang, Ying and Schriver, Kenneth E. and Hu, Jia Ming and Roe, Anna Wang},
  journal = {eLife},
  year    = {2023},
  volume  = {12},
  pages   = {e81794},
  doi     = {10.7554/eLife.81794}
}

@article{Lu2018V1V4,
  title   = {Revealing Detail along the Visual Hierarchy: Neural Clustering Preserves Acuity from V1 to V4},
  author  = {Lu, Yiliang and Yin, Jiapeng and Chen, Zheyuan and Gong, Hongliang and Andolina, Ian M. and Shipp, Stewart and Wang, Weidong and Roe, Anna Wang},
  journal = {Neuron},
  year    = {2018},
  volume  = {98},
  number  = {2},
  pages   = {417--428.e3},
  doi     = {10.1016/j.neuron.2018.03.009}
}

@techreport{SerreRiesenhuber2004,
  title       = {Realistic Modeling of Simple and Complex Cell Tuning in the HMAX Model, and Implications for Invariant Object Recognition in Cortex},
  author      = {Serre, Thomas and Riesenhuber, Maximilian},
  institution = {MIT Computer Science and Artificial Intelligence Laboratory},
  year        = {2004},
  number      = {MIT-CSAIL-TR-2004-052}
}

@misc{Witkin1983,
  title     = {Scale-space filtering},
  author    = {Witkin, Andrew P.},
  booktitle = {Proceedings of the 8th International Joint Conference on Artificial Intelligence (IJCAI-83)},
  year      = {1983},
  pages     = {1019--1022},
  note      = {Karlsruhe, West Germany}
}

@article{Koenderink1984,
  title   = {The structure of images},
  author  = {Koenderink, Jan J.},
  journal = {Biological Cybernetics},
  year    = {1984},
  volume  = {50},
  pages   = {363--370},
  doi     = {10.1007/BF00336961}
}

@book{Lindeberg1994,
  title     = {Scale-Space Theory in Computer Vision},
  author    = {Lindeberg, Tony},
  publisher = {Springer},
  address   = {New York, NY},
  year      = {1994},
  doi       = {10.1007/978-1-4757-6465-9},
  series    = {The Springer International Series in Engineering and Computer Science}
}

@article{Lindeberg1998,
  title   = {Feature detection with automatic scale selection},
  author  = {Lindeberg, Tony},
  journal = {International Journal of Computer Vision},
  year    = {1998},
  volume  = {30},
  number  = {2},
  pages   = {79--116},
  doi     = {10.1023/A:1008045108935}
}

@article{Lindeberg2013,
  title   = {A computational theory of visual receptive fields},
  author  = {Lindeberg, Tony},
  journal = {Biological Cybernetics},
  year    = {2013},
  volume  = {107},
  number  = {6},
  pages   = {589--635},
  doi     = {10.1007/s00422-013-0569-z}
}

@article{Lu2025AllTNN,
  title   = {End-to-end topographic networks as models of cortical map formation and human visual behaviour},
  author  = {Lu, Zejin and Doerig, Adrien and Bosch, Victoria and Krahmer, Bas and Kaiser, Daniel and Cichy, Radoslaw M. and Kietzmann, Tim C.},
  journal = {Nature Human Behaviour},
  year    = {2025},
  volume  = {9},
  pages   = {1975--1991},
  doi     = {10.1038/s41562-025-02220-7}
}

@article{Blauch2022ITN,
  title   = {A connectivity-constrained computational account of topographic organization in primate high-level visual cortex},
  author  = {Blauch, Nicholas M. and Behrmann, Marlene and Plaut, David C.},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  year    = {2022},
  volume  = {119},
  number  = {3},
  pages   = {e2112566119},
  doi     = {10.1073/pnas.2112566119}
}

@article{Margalit2024TDANN,
  title   = {A unifying framework for functional organization in early and higher ventral visual cortex},
  author  = {Margalit, Eshed and Lee, Hyodong and Finzi, Dawn and DiCarlo, James J. and Grill-Spector, Kalanit and Yamins, Daniel L. K.},
  journal = {Neuron},
  year    = {2024},
  volume  = {112},
  number  = {14},
  pages   = {2435--2451.e7},
  doi     = {10.1016/j.neuron.2024.04.018}
}

@article{Araujo2019computing,
  title   = {Computing Receptive Fields of Convolutional Neural Networks},
  author  = {Araujo, Andre and Norris, Wade and Sim, Jack},
  journal = {Distill},
  year    = {2019},
  url     = {https://distill.pub/2019/computing-receptive-fields}
}

@article{PasupathyPopovkinaKim2020V4Review,
  title   = {Visual Functions of Primate Area {V4}},
  author  = {Pasupathy, Anitha and Popovkina, Dina V. and Kim, Taekjun},
  journal = {Annual Review of Vision Science},
  year    = {2020},
  volume  = {6},
  pages   = {363--385},
  doi     = {10.1146/annurev-vision-030320-041306}
}

@article{HarShalomWeiss2025VNNReview,
  title        = {What Do Visual Neural Networks Learn?},
  author       = {Har-Shalom, Daniella and Weiss, Yair},
  journal      = {Annual Review of Vision Science},
  year         = {2025},
  volume       = {11},
  number       = {1},
  pages        = {591--610},
  doi          = {10.1146/annurev-vision-110323-112903}
}

@misc{WichmannGeirhos2023DNNBehaviorReview,
  title        = {Are Deep Neural Networks Adequate Behavioural Models of Human Visual Perception?},
  author       = {Wichmann, Felix A. and Geirhos, Robert},
  year         = {2023},
  howpublished = {arXiv preprint},
  eprint       = {2305.17023},
  archivePrefix= {arXiv},
  primaryClass = {cs.CV}
}

@article{Kheradpisheh2016VariationHarder,
  title        = {Humans and Deep Networks Largely Agree on Which Kinds of Variation Make Object Recognition Harder},
  author       = {Kheradpisheh, Saeed Reza and Ghodrati, Masoud and Ganjtabesh, Mohammad and Masquelier, Timoth{\'e}e},
  journal      = {Frontiers in Computational Neuroscience},
  year         = {2016},
  volume       = {10},
  pages        = {92},
  doi          = {10.3389/fncom.2016.00092}
}

@article{XuVaziriPashkam2022Tolerance,
  title        = {Understanding transformation tolerant visual object representations in the human brain and convolutional neural networks},
  author       = {Xu, Yaoda and Vaziri-Pashkam, Maryam},
  journal      = {NeuroImage},
  year         = {2022},
  volume       = {263},
  pages        = {119635},
  doi          = {10.1016/j.neuroimage.2022.119635}
}

@misc{KumarCluneLehmanStanley2025FER,
  title        = {Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis},
  author       = {Kumar, Akarsh and Clune, Jeff and Lehman, Joel and Stanley, Kenneth O.},
  year         = {2025},
  howpublished = {arXiv preprint},
  eprint       = {2505.11581},
  archivePrefix= {arXiv},
  primaryClass = {cs.CV},
  doi          = {10.48550/arXiv.2505.11581}
}
