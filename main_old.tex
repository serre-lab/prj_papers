%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Identification of unknown leaf fossils using explainable deep learning}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

\author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

\affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Isolated leaves dominate the angiosperm fossil record, but correctly identifying them remains one of the most challenging problems in paleobotany. Here, we present a novel AI-based approach to assist paleobotanists with the family-level classification of fossil leaves. Our approach leverages modern computer vision methods to reduce biases from image features unrelated to the leaves, such as manual annotations and rulers3 in an extensive database of cleared leaf images, which is then used to train a deep neural network for family classification. Using newly developed explainability methods, we show that the corresponding model learns meaningful leaf features with a level of accuracy of XX\% (XX-way classification; chance level: XX\%). Further leveraging generative AI, we augment our image datasets of extant leaves with synthetic, highly photorealistic fossil counterparts, enhancing the familial diversity of images analyzable as fossils. We show that a deep neural network model trained on this extended dataset using a custom training objective to enforce consistency across the leaf vs. fossil domains for single-family classification can reliably identify vetted fossil leaves -- even for families that lacked real fossils for training. The model is further applied to the Florissant Fossil Beds National Monument, late Eocene of Colorado, to classify XX unknowns. This work demonstrates the potential for AI to assist paleobotanists...}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{keyword1, Keyword2, Keyword3, Keyword4}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Main}\label{main}

PETER TO ADD A BLURB FOR THE INTRO


As you all know, Deep learning has ignited a revolution in AI -- opening up a myriad of
applications. From large language models and chatbots like chatGPT, to art with
generative AI, to science, as seen throughout this session and including live-plant botany.
Many of you are familiar with nature apps like PlantNet and iNaturalist. Plant Village,
founded by my colleague David Hughes at Penn State, literally puts identification of plant
diseases into the hands of the world’s farmers. However, dead-plant botany, our topic
today, is a far more challenging problem.


Isolated leaves are by far the most common plant fossils, but they are famously difficult to
identify correctly, making them paleobotany’s undertapped galaxy of Dark Data. The
older literature is littered with thousands of botanically incorrect names. The reasons for
this situation are well known and include the extreme complexity of leaf shape and
venation, a pittance of vetted training samples - in comparison to microfossils for
example- and the massive variation within categories and among fossil sites.
The first issue, complexity, is a perfect venue for AI assistance. The other two issues, low
sample size and variation, are as difficult for machines as humans. How can we work
around them?


To limit variation, we strategically focused on the single site with by far the largest sample
of photographed and vetted leaf fossils in the world – the world-famous Florissant Fossil
Beds from the late Eocene of Colorado. The Florissant flora is one of the best studied of
the Cenozoic, including Harry Macginitie’s renowned 1953 monograph and dozens of
state-of-the-art systematics papers by Steven Manchester and many others. This backbone
of prior work allows confident placement of several thousand imaged fossils from a single site into extant plant families, while several thousand more remain unidentified.
Importantly, testing our methods on Florissant will create the foundation for cross-
training the system to accommodate other fossil sites.

5.
Herb Meyer, just retired from the National Park Service, and dozens of his interns spent
many years photographing Florissant fossils in collections around the world. This was
probably the first large image database for paleontology of any kind and remains
available on the Internet. For today’s purposes, we are using his images from 16 plant
families that have at least 5 specimens each.

6.
The Florissant images from Herb Meyer are also part of a much larger dataset of fossil,
cleared, and x-rayed leaf images from numerous legacy sources that we compiled, vetted,
and published recently in PhytoKeys, posted open-access on Figshare Plus, and just
updated a few weeks ago to include 4k more cleared-leaf images. All of this is available to
you with a single click! We encourage all of you interested in leaves and leaf fossils to
download this amazing and easy to use resource, which I use almost every day to help
identify leaf fossils, using my old-school human vision.

7.
From the extant, cleared and x-rayed leaves in the dataset, we here used 142 families that
had 25 or more specimens each for training and cross-training with the fossils of 16
families from Florissant.

8.
Now we face the challenge of sample size, which as you can see is especially limited for
fossil leaves. The Catch 22 is that the fossil leaves we want to identify usually represent
families with no training fossils at all. Like humans, the machine has to identify these with
only extant leaves as a reference. When we paleobotanists do this exercise, we
unconsciously bridge the enormous visual gap between extant and fossil specimens and
make our comparisons. Our solution is for the machine to mimic this process by making
synthetic fossils from the extant images using generative AI methods. As you can see, the
sample size of images analyzable as fossils increases enormously as a result.

9.
Here are some examples of real and deep-fake fossils, can you tell which are which?
We have heard a lot of negative press about deep fakes in the media, for good reason, but
in this case deep fakes provide an obvious benefit to work around a significant scientific
obstacle. You can see the huge increase in sample size and diversity for training images.
So, let’s go through a series of scenarios to see how well this works.

10.
As a base model we used a type of convolutional neural network called a resnet-101 (101 layers deep),
pretrained on natural images (imageNet). We considered alternatives including pre-training on herbarium sheets from the FGVC9 challenge\cite{ref} but obtained worse results. We also trained and evaluated a transformer network based on the BEiT model~\citep{ref}.
All reported results correspond to model accuracies as Top-5, meaning that the system found the correct plant family
among its five most probable plant families. The chance level for this scenario was $3.5\%$ ($5/142$).
The training/validation/test split was $80\%/10\%/10\%$. Grid search was used across all hyperparameters (including learning rate and regularization) and selected based on average accuracy on the validation set. Reported test accuracy is based on average across all splits. 


One challenge in evaluating the accuracy of the system is that we do not have real vetted fossils for most classes (i.e., for 142-16=126 classes). Here, we only evaluate the accuracy of the system on real fossils. We try to provide  a lower and upper bound on the expected accuracy of the system based on whether or not real fossils were included in the training for the families used during test, we evaluated the system on families that contained real fossils in the training set.

Although the system makes predictions for all 142 families the system can only be evaluated on the 16 families for which we have vetted real fossils. To provide and upper/lower bounds we trained a system with/without real fossils available for each of the 16 classes (leaving one family out and averaging over all permutations corresponding to training of 16 distinct models). 

The average accuracy of this system is $91.8\%$ (Brown dashed line; Figure~\ref{fig:system})a). Average, lower and upper bounds are shown in Figure~\ref{fig:system}. To further validate the system we performed an explainability analysis based on the saliency method~\ref{} using the Xplique toolbox~\citep{} developed by our group. We noticed that the system learned shortcuts and leveraged textual and other cues (e.g., ruler) present on the cleared leaf slides.

To prevent the system from leveraging these shortcuts we developed an approach to clean up the image database. We fine-tuned a standard object segmentation algorithm called `segment anything model'' (SAM)~\cite{kirillov2023segment} on a small set of $576$ manually annotated cleared leaf images. Visual inspection on the remaining images from the database revealed that the system worked well at segmenting out leaves from the background. We ran SAM on the entire dataset and masked out the background (Figure~\ref{fig:system}b). 
We confirmed that the accuracy of the identification system remained high (yellow in Figure~\ref{fig:system}a), and most importantly, visual inspection of distribution maps for the system confirmed that it relies on the leaf (see SI).

We further hypothesized that leaf contains information at multiple scales from the coarsest scales where shape can be encoded to the finest details associated with the pattern of leaf venation. We thus extended our base architecture to process images at multiple scales. For each image, we created 5 samples at different scales which included: (i) the original masked image, a tighter crop of the image corresponding to the smallest bounding box to include the mask, as well as three additional crops corresponding to the top, middle and bottom one-third of the leaf image (see SI).  Again, we confirmed that this extension improved further the accuracy of the system (ADD).

Our database includes a disproportionate number of cleared leaves to fossils ratio ($34328/3200 \simeq 10:1$). To evaluate the impact of the imbalance we calculated the average distance within the class between leaves and between leaves and fossils normalized by the average distance between class for leaves and fossils. We found that on average fossils were further away from leaves than typical leaves. This is due to differences in appearance between leaves and fossils (for instance the presence of a rock, etc). To try to remediate this issue we added a regularization term to our initial classification loss which we will refer to as the ``triplet loss''. The idea is to enforce a penalty during training to encourage fossil and cleared leaves from the same class to be closer together than fossil and leaves across classes. See Methods (Eq.~\ref{} for details). This led to a significant improvement in the accuracy of the system, especially for classes for which no real (vetted) fossil was available during training for that class (purple color; Figure~\ref{fig:system}A.).

The final challenge to address is a complete lack of fossils for most families in our dataset ($119/142$) in addition to seven families that contained less than five samples. To address this challenge, we turned to generative AI. We leverage a state-of-the-art stable diffusion model based on the ControlNet approach~\citep{zhang2023adding}. The original model is a conditional diffusion model that generates controlled images using text prompts. This open-source model was trained on LAION 5 billion dataset. Here, we trained the ControlNet to generate synthetic cleared leaves and fossils. The ControlNet was trained end-to-end together with the classifier by minimizing one joint loss aimed at simultaneously classifying cleared and fossil leaf images correctly together with the triplet loss to encourage good family representations of cleared and fossil leaf images together with the ControlNet loss. Representative sample synthetic fossils are shown in Figure~\ref{fig:system}. Synthesizing fossils in this way yields massive improvement in the system's ability to identify leaves for which real fossils are not available for training, improving accuracy from just above chance ($4.31\%$) (Figure~\ref{fig:system} A lower-bound) to $77.3\%$ (upper-bound).
        


% trained  the system on all real : real fossils are available during training
% 2) Lower bound: no real fossils of the test family are available during training. This
% scenario is the closest to real-life paleobotany, where we are often identifying an
% unknown fossil with various extant material but no related fossils for reference.
% This is a baseline scenario without synthetic fossils or other enhancements. As you can see,
% the system’s accuracy was near chance when no fossils were used for training.

% 11.
% The next enhancement was segmenting the images to remove distractors and “clues” such
% as text and scales. This only provided a small boost in accuracy,

% 12.
% The next enhancement directly addresses the issue of fossil and modern leaves, even from
% the same family, occupying entirely different image spaces, as you can see in the two-
% dimensional projections of the visual layers at left. This is why simply uploading a fossil
% leaf into PlantNet, as many of us attempt in desperation, never works.
% This situation is bad because it means that the network learns different strategies to
% classify extant and fossil leaves. To improve the situation, we enforce a smoothness term

% that guarantees that, within families, extant leaves and fossils “mix”. This is done by
% forcing a fossil and extant leaf from the same family to be closer to each other than the
% extant leaf and an extant leaf from a different family, or similarly for 2 different fossils
% from 2 different families. This approach is also likely to help in the future as we integrate
% more sites beyond Florissant.
% At the lower left, you can see that the method works because you can no longer see
% separate extant-fossil clusters.
% At the right you can see the first significant boost in accuracy.


% 13.
% At this stage we are using the synthetic fossils previously discussed, and as you can see
% there is a huge improvement to nearly 80% top-5 accuracy, even under the lower-bound
% scenario where no real fossils were used for training.
% To recap, we are in a new place now: the system is now providing thousands of accurate
% identifications of real Florissant fossils, with 142 possible choices each time, for families it
% has only seen as extant specimens or synthetic fossils.

% 14.
% Visualizations and heat maps of what the computer finds diagnostic, or explainability, are
% highly botanically informative, as Eddie Spagnuolo et al. showed in their recently
% published analysis of an older set of heat maps from our project. There are many
% techniques for generating heat maps, and these here were done using saliency, which
% measures the sensitivity of the output to tiny perturbations at each pixel location. At a
% glance, you can see activations varying by family from features such as midveins, teeth,
% and so on.

% 15.
% CRAFT is a new technique developed in the Serre lab to identify key features. Here, in the
% family Salicaceae, feature 1 shows activations of various vein patterns.

% 16.
% MACO is another explainability method developed in the Serre lab. The image on the
% right is a maximally activating stimulus for “Salicaceae” as a whole, in a single image. We

% used optimization methods to synthesize an image that activates the unit as much as
% possible.
% You can quickly see that the image successfully displays never-described family-level
% features of marginal venation, curvature, and serration.

% 17.
% Here is the second most important visual feature for Salicaceae, at 12% importance,
% rendered with both visualization methods. This feature is very strongly associated with
% marginal serration.

% 18.
% The methods described today will soon be available in a simple web tool via image upload.
% Here is a gorgeous Juglandaceae specimen just loaded.

% 19.
% And here are the resulting outputs, including a list of the top family matches, family
% distribution of the 200 closest images in the database, heat maps, and the five closest
% images in the dataset.
% The tool is calibrated only to Florissant so far, but of course there is no reason not to try it
% on any fossil.

% 20.
% Even at Florissant, there is an immediate use case because there are thousands of fossils
% that are either published, but with questionable identifications by modern standards, or
% not classified at all. Here are some examples. As always, the machine is only an assistant
% making suggestions. But we paleobotanists can incorporate the suggestions to get closer to
% the correct identification.

% 21.
% In summary,
% We presented the first AI assistant for paleobotany, which will be released with the
% corresponding article.

% We worked around the sample size limitations by successfully using synthetic fossils,
% resulting in correct identifications for families even when no fossil training data were
% available.
% State of the art visualization techniques provide novel botanical insights into the defining
% family-level characteristics seen in living and fossil leaves.
% Cross training and further development will allow expansion to include more fossil sites.
% In the meantime, there is an immediate use case to improve identifications for the many
% Florissant unknowns.

% %%%








% %%%
% Here, we present a novel AI-based approach to assist paleobotanists with the family-level classification of fossil leaves. Our approach leverages modern computer vision methods to reduce biases from image features unrelated to the leaves, such as manual annotations and rulers3 in an extensive database of cleared leaf images, which is then used to train a deep neural network for family classification. Using newly developed explainability methods, we show that the corresponding model learns meaningful leaf features with a level of accuracy of XX\% (XX-way classification; chance level: XX\%). Further leveraging generative AI, we augment our image datasets of extant leaves with synthetic, highly photorealistic fossil counterparts, enhancing the familial diversity of images analyzable as fossils. We show that a deep neural network model trained on this extended dataset using a custom training objective to enforce consistency across the leaf vs. fossil domains for single-family classification can reliably identify vetted fossil leaves -- even for families that lacked real fossils for training. The model is further applied to the Florissant Fossil Beds National Monument, late Eocene of Colorado, to classify XX unknowns. 


\section{Training an interpretable deep learning model for leaf classification}\label{sec2}

% We have developed a deep-learning-based computer-vision system for identifying extant-leaf images of botanical families using a new image database of cleared, x-rayed, and fossil leaves consisting mainly of angiosperms. Here, we describe novel methods to extend the system for identifying fossil leaves at the family level. 

% The challenge for the development of computer vision systems, which normally rely on tens of thousands to millions of training images, is that comparatively few vetted fossil leaf samples are available to train the system; the majority of angiosperm families have no reliable leaf fossils. Here, we describe the development of computer-vision methods to successfully transfer machine knowledge from cleared to fossil leaves. Our approach leverages style transfer or image to image translation,  to generate synthetic fossils by learning mappings between one image distribution (cleared leaves) and another (fossil leaves). We use these methods to augment our real-image database with a high quantity and phylogenetic diversity of synthetic samples not available from real fossils alone. We train a deep neural network architecture using both real and synthetic images to learn a joint representation for known families of cleared leaves and fossils.

% We evaluate the network’s accuracy in multiple scenarios. First, we demonstrate high classification accuracy for cleared leaves. We further find a high (albeit lower) accuracy for real fossil leaves. This is presumably due to the comparatively much smaller number of fossil vs. cleared-leaf samples, combined with taphonomic signal loss. We further evaluate the ability of the proposed methods to generalize to real fossils of families for which no real fossil was presented during training. We use a leave-one-family-out cross-validation approach whereby real leaf fossils are used for training for all families but one (i.e., only synthetic samples are available for the test family). We report significantly above-chance classification accuracy in this scenario. 

% A study using explainability methods is carried out in order to identify some of the strategies used for the classification. Our results strongly suggest that AI methods will provide significant assistance to paleobotanists with the identification of leaf fossils.

\section{Interpreting how the model learns to classify leaf images}

While we do not have human accuracy data for comparison, it is pretty clear that the system is capable of identifying leaves far better than any human would. In order to understand the strategy used by the network, our group has developed a number of methods to try to better characterize the strategies used by the network~\cite{craft, holistic, maco}. Examples of corresponding visualizations for sample families are shown in Fig.~\ref{}. 
Start with leaf concepts learned using overcomplete dictionary learning approach to discover "leaf concepts" corresponding to leaf components that are driving predictions by the model. We ran a dictionary learning method (which?) tcanv



\section{Application to the classification of Florissant leaf fossil images}

\section{Conclusion}

\section{Methods}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figure1_v3.jpeg}
    \caption{Caption}
    \label{fig:system}
\end{figure}
Our system combines different state of the art computer vision models to segment, generate and classify images.  

Semantic segmentation via Segment Anything (SAM) \cite{kirillov2023segment}  is used to clean images of the cleared leaves collection that have annotations, rulers and other class independent information that may provide shortcuts for the system. 

Given the extreme lack of vetted fossil samples in our dataset, we adopt a generative approach where we aim to generate photo-realistic fossil-looking leaves using as base the vetted cleared leafs dataset counterpart. We use a cycle-Controlnet based on stable diffusion  \cite{zhang2023adding},  a generative  method that achieves incredibly realistic image and Cycle-consistent generative approaches that have been used for domain adaption  and style transfer\cite{CycleGAN2017}. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figure2_v1.jpg}
    \caption{Caption}
    \label{fig:system}
\end{figure}
Finally,  we use a state of the art classification model BEIT \cite{bao2022beit}. Taking inspiration from \citep{taha2020triplet} we  further equip the model  with a triplet-loss regularizer that forces samples of the same family to be close to each other in the feature space. 



\subsection{ Dataset}

For the rest of the paper we will consider two domains: the \textbf{Leaves domain} and  the \textbf{Fossil domain}.  We will also refer to  leaves that have been transformed to the Fossil domain via the generative process as \textbf{Synthetic Leaves}.

The Leaves domain refers to the set of 30252 images of vetted cleared (and x-rayed) leaves published on \cite{wilf2021dataset}, including 4076 newly added samples from the National Museum of Nature and Science (Ibaraki, Japan). The Fossils domain is a set from the Florissant collection, amassed by \citep{florissant},probably the largest of its kind. \citep{wilf2021dataset} made available a subset used here of 3,200 Florissant images vetted to 23  families, from which we will use 16 (all the families where there are at least 5 specimens).  There are around 1,000 specimens with unknown labels. 



\subsection{Shortcut Removal Via SAM}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figure_segmentation.jpg}
%     \caption{Result of applying SAM to images from the x-ray cleared leaf collection}
%     \label{fig:segmentation}
% \end{figure}
%\figure with cleared leaveS? 

Our goal is to leverage the high amount of samples available from the Leaves Domain, however there are multiple artifacts that can bias our model. For instance, hand-written labels, rulers and  other stains product of the handling of the samples. For this reason we leverage the usage of SAM.  We start by fine-tunning the model with a small subset of 500 cleared leaves with manual annotations. Our model reaches 95\% IOU, on a 80-20 split. This is then applied to the rest of the cleared leaves dataset as a pre-processing step. Refer to Figure~\ref{fig:segmentation} for examples. 

\subsection{Style Transfer/ Domain Adaptation}%\label{sec2.2}

Let us consider   the fossil domain  (X) and the  leaves domain (Y), with distinctive image properties but with some specific features that can be identified by automatic pipelines \cite{wilf2016leaves}. Now the amount of samples of X is considerably much lower than Y, in fact for every family in Y there is no guarantee, that there will be a a counter part in X.  

We take inspiration from \cite{CycleGAN2017} and adopt a generative approach. We have two mappings  $G: X \rightarrow Y$ and $F: Y \rightarrow X$. Where  $G$ and $F$ are controlnet \cite{zhang2023adding} modules with fixed random seed  that can be guided through text. In our case we will use  the prompts: ``A cleared leaf of the family: $<$ Family Name $>$'' and ``A fossilized leaf of the family : $<$Family Name$>$''  to guide the generation. 

Since our end goal is not only to generate fossilized looking samples but to encode family traits that can be used for classification, we use a triplet regularizer (T) to enforce proximity between elements of the same family and larger distance from samples of different families. The triplet regularizer is train at every cycle of our generator, ensuring that at the source (T(X)), the target (T(Y)), and the transformations (T(G(X)),T(F(Y))) the latent structure is keeping relevant information for  classification. As well, in order to ensure that there is preservation of shape during the generative process of the diffusion model , we use our SAM  module to ensure that  the outputs maintain similar  shape from the inputs. 

% The final objective function then can be written as: 

% \begin{align}
%     \mathcal{L}(G,F) &= \mathcal{L}_{GAN}(G,D_Y,X,Y)  
%     \\&+ \mathcal{L}_{GAN}(G,D_X,Y,X) 
%     \\&+ \lambda_1 \mathcal{L}(G,F) 
%     \\&+ \lambda_2 \mathcal{L}(T,G,F,X,Y)
% \end{align}



\subsection{Triplet Loss and Forcing structure}\label{sec2.2}

Following \cite{taha2020triplet} we adapt our BEiT model with two heads, a classification head and an embedding head. The classification layer is trained using cross-entropy loss and the embedding head using the triplet loss. The triplet loss would  perform  sampling using the information of the domains (fossil or extant), taking one class from both domains for anchors, positive and negative sampling, thus  ensuring that in the embedding there is a local and global structure. 

To summarize, we formulate our classification loss in the following way. 

\begin{align}
    \mathcal{L}_{trip}= \frac{1}{b}\sum_{i=1}^{b}[(D(a_{i,x},p_{i,x})-D(a_{i,x}x,n_{i,x}) + m)]
\end{align}

Where b is the number of samplings and D is a distance function that acts over an anchor (a) a positive sample (p) and a negative sample (n) with a fixed margin (m); with $x \in \{X,Y\}$ the two domains that we are considering. 

Then the total loss function on which this network is trained can be written as: 
\begin{equation}
    \mathcal{L} = \mathcal{L}_{softmax} + \lambda \mathcal{L}_{trip}
\end{equation}

Finally, we added the synthetic images created in the first step to account for those classes where little or no  data is available. 

\subsection{Explainability}

\ivan{Need help from T. Fel and Gaurav}



\subsection{Metrics}

To evaluate our method, we used two scenarios providing and upper-bound and lower bound of accuracy. In the first one we  use a 90-10 split cross validation, where  real fossils are used training and testing. The second one resembles the closest real-life paleobotany, where we take a “one left out” approach. In this case,  no real fossils are used to train the test family. For the rest of the families 90\% of real fossils are provided but we only evaluate on the test family,then we repeat for all the families and provide a score. 

\section{Results}

\ivan{Working on a visual for Fossils }

First,  we created around 30000 synthetic images corresponding to all the available cleared leaves in the 16 classes considered. See in  \ivan{pending fossils figure }. Then we  classify using the  scenarios described before. 

Our results on the lower and upper bound scenarios suggest that using our triplet loss to enforce structure helps in classification across domain. As well, the generation of photo realistic synthetic fossils provide crucial help in the classification of fossils that  have not been seen during training, as we can appreciate in Table ~\ref{tab:beit}. 


 
 

\begin{table}[ht!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
Condition &
  \begin{tabular}[c]{@{}c@{}}Top 5\\ Lower Bound\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Top 5 \\ Upper Bound\end{tabular} &
  \#Classes \\ \hline
Unsegmented                                                   & 4.21 \% & 64.3 \% & 142 \\ \hline
Segmented                                                     & 6.21 \% & 65.1 \% & 142 \\ \hline
\begin{tabular}[c]{@{}c@{}}Triplet +\\ Segmented\end{tabular} & 21.4 \% & 72 \%   & 142 \\ \hline
\begin{tabular}[c]{@{}c@{}}Triplet + \\ Segmented +\\ Synthetic Fossils\end{tabular} &
  \textbf{75.1 \%} &
  \textbf{86.4 \%} &
  142 \\ \hline
\end{tabular}%
}
\caption{BeIT Classification results under the different conditions studied. }
\label{tab:beit}
\end{table}

\fel{(3)(b) Look at our beautiful embedding (with / without triplet).}
... .

\ivan{(4) look to our amazing explanations, @Gaurav is helping with the figure.}
 


\Ivan {(5) We demonstrate how the model helps the expert to identify unknown fossils. Once we get approval from the last model, we can calculate the predictions and send them to Peter et al. and we can report those results }
In order to understand the interactions and characteristics that lead the model to make a decision, we studied the explainability maps after training (see Fig3).


\section{Conclusion}

\ivan{starting from the conclusions of the presentation}
\begin{itemize}
    \item First virtual AI assistant for macrofossil paleobotany
    \item Uses state-of-the-art deep generative methods to generate synthetic leaf fossils from extant leaves, vastly increasing “fossil” training sample size and accuracy of fossil identifications.
    \item  Identifies fossils correctly to families for which no fossil specimens were available during training.
    \item Immediate use case to help identify thousands of Florissant unknowns.
    \item Development with Florissant is a significant step towards more general applications for other floras. 

\end{itemize}

\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{SI Appendix: Shortcut Removal via SAM}

We conducted a qualitative evaluation of the attribution maps generated by our Baseline and Segmented models using RISE (Randomized Input Sampling for Explanation) ~\citep{RISE2018} and GradCAM (Gradient-weighted Class Activation Mapping) ~\citep{Selvaraju_2019}. These methods were implemented using the Xplique Toolbox ~\citep{fel2022xplique}, developed in our lab. Our findings indicate that the attribution maps of the Segmented model, trained with segmentation, predominantly focus on the leaf regions within the image, demonstrating alignment with relevant features. In contrast, the Baseline model exhibited attribution patterns that relied on spurious shortcuts, such as text and rulers present in the images, as illustrated in Figure~\ref{fig:system2} and ~\ref{fig:system3}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{RISE2.png}
    \caption{Caption}
    \label{fig:system2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{GradCAM2.png}
    \caption{Caption}
    \label{fig:system3}
\end{figure}


\bmhead{Acknowledgements}

Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

Please refer to Journal-level guidance for any specific requirements.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}

\section{Section title of first appendix}\label{secA1}

An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%
\bibliographystyle{sn-mathphys}
\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
